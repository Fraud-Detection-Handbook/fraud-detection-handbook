{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(shared_functions)=\n",
    "# Shared functions\n",
    "\n",
    "This notebook contains functions with are commonly reused in the book, such as for loading and saving data, fitting and assessing prediction models, or plotting results. \n",
    "\n",
    "The notebook can be downloaded from GitHub with\n",
    "\n",
    "```\n",
    "!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/Chapter_References/shared_functions.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "The notebook can be included in other notebooks using\n",
    "\n",
    "```\n",
    "%run shared_functions\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "#import sklearn\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "import graphviz\n",
    "import xgboost\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_from_files\n",
    "\n",
    "First use in [Chapter 3, Baseline Feature Transformation](Baseline_Feature_Transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of pickle files, put them together in a single dataframe, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    \n",
    "    files = [os.path.join(DIR_INPUT, f) for f in os.listdir(DIR_INPUT) if f>=BEGIN_DATE+'.pkl' and f<=END_DATE+'.pkl']\n",
    "\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('TRANSACTION_ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save oject as pickle file\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaleData\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaleData(train,test,features):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(train[features])\n",
    "    train[features]=scaler.transform(train[features])\n",
    "    test[features]=scaler.transform(test[features])\n",
    "    \n",
    "    return (train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test splitting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_train_test_set\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "Sampling ratio added in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_set(transactions_df,\n",
    "                       start_date_training,\n",
    "                       delta_train=7,delta_delay=7,delta_test=7,\n",
    "                       sampling_ratio=1.0,\n",
    "                       random_state=0):\n",
    "    \n",
    "    # Get the training set data\n",
    "    train_df = transactions_df[(transactions_df.TX_DATETIME>=start_date_training) &\n",
    "                               (transactions_df.TX_DATETIME<start_date_training+datetime.timedelta(days=delta_train))]\n",
    "    \n",
    "    # Get the test set data\n",
    "    test_df = []\n",
    "    \n",
    "    # Note: Cards known to be frauded after the delay period are removed from the test set\n",
    "    # That is, for each test day, all frauds known at (test_day-delay_period) are removed\n",
    "    \n",
    "    # First, get known frauded customers from the training set\n",
    "    known_frauded_customers = set(train_df[train_df.TX_FRAUD==1].CUSTOMER_ID)\n",
    "    \n",
    "    # Get the relative starting day of training set (easier than TX_DATETIME to collect test data)\n",
    "    start_tx_time_days_training = train_df.TX_TIME_DAYS.min()\n",
    "    \n",
    "    # Then, for each day of the test set\n",
    "    for day in range(delta_test):\n",
    "    \n",
    "        # Get test data for that day\n",
    "        test_df_day = transactions_df[transactions_df.TX_TIME_DAYS==start_tx_time_days_training+\n",
    "                                                                    delta_train+delta_delay+\n",
    "                                                                    day]\n",
    "        \n",
    "        # Frauded cards from that test day, minus the delay period, are added to the pool of known frauded customers\n",
    "        test_df_day_delay_period = transactions_df[transactions_df.TX_TIME_DAYS==start_tx_time_days_training+\n",
    "                                                                                delta_train+\n",
    "                                                                                day-1]\n",
    "        \n",
    "        new_frauded_customers = set(test_df_day_delay_period[test_df_day_delay_period.TX_FRAUD==1].CUSTOMER_ID)\n",
    "        known_frauded_customers = known_frauded_customers.union(new_frauded_customers)\n",
    "        \n",
    "        test_df_day = test_df_day[~test_df_day.CUSTOMER_ID.isin(known_frauded_customers)]\n",
    "        \n",
    "        test_df.append(test_df_day)\n",
    "        \n",
    "    test_df = pd.concat(test_df)\n",
    "    \n",
    "    # If subsample\n",
    "    if sampling_ratio<1:\n",
    "        \n",
    "        train_df_frauds=train_df[train_df.TX_FRAUD==1].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df_genuine=train_df[train_df.TX_FRAUD==0].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df=pd.concat([train_df_frauds,train_df_genuine])\n",
    "        \n",
    "    # Sort data sets by ascending order of transaction ID\n",
    "    train_df=train_df.sort_values('TRANSACTION_ID')\n",
    "    test_df=test_df.sort_values('TRANSACTION_ID')\n",
    "    \n",
    "    return (train_df, test_df)\n",
    "                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prequentialSplit\n",
    "\n",
    "First use in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequentialSplit(transactions_df,\n",
    "                     start_date_training, \n",
    "                     n_folds=4, \n",
    "                     delta_train=7,\n",
    "                     delta_delay=7,\n",
    "                     delta_assessment=7):\n",
    "    \n",
    "    prequential_split_indices=[]\n",
    "        \n",
    "    # For each fold\n",
    "    for fold in range(n_folds):\n",
    "        \n",
    "        # Shift back start date for training by the fold index times the assessment period (delta_assessment)\n",
    "        # (See Fig. 5)\n",
    "        start_date_training_fold = start_date_training-datetime.timedelta(days=fold*delta_assessment)\n",
    "        \n",
    "        # Get the training and test (assessment) sets\n",
    "        (train_df, test_df)=get_train_test_set(transactions_df,\n",
    "                                               start_date_training=start_date_training_fold,\n",
    "                                               delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_assessment)\n",
    "    \n",
    "        # Get the indices from the two sets, and add them to the list of prequential splits\n",
    "        indices_train=list(train_df.index)\n",
    "        indices_test=list(test_df.index)\n",
    "        \n",
    "        prequential_split_indices.append((indices_train,indices_test))\n",
    "    \n",
    "    return prequential_split_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_model_and_get_predictions\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_and_get_predictions(classifier, train_df, test_df, \n",
    "                                  input_features, output_feature=\"TX_FRAUD\",scale=True):\n",
    "\n",
    "    # By default, scales input data\n",
    "    if scale:\n",
    "        (train_df, test_df)=scaleData(train_df,test_df,input_features)\n",
    "    \n",
    "    # We first train the classifier using the `fit` method, and pass as arguments the input and output features\n",
    "    start_time=time.time()\n",
    "    classifier.fit(train_df[input_features], train_df[output_feature])\n",
    "    training_execution_time=time.time()-start_time\n",
    "\n",
    "    # We then get the predictions on the training and test data using the `predict_proba` method\n",
    "    # The predictions are returned as a numpy array, that provides the probability of fraud for each transaction \n",
    "    start_time=time.time()\n",
    "    predictions_test=classifier.predict_proba(test_df[input_features])[:,1]\n",
    "    prediction_execution_time=time.time()-start_time\n",
    "    \n",
    "    predictions_train=classifier.predict_proba(train_df[input_features])[:,1]\n",
    "\n",
    "    # The result is returned as a dictionary containing the fitted models, \n",
    "    # and the predictions on the training and test sets\n",
    "    model_and_predictions_dictionary = {'classifier': classifier,\n",
    "                                        'predictions_test': predictions_test,\n",
    "                                        'predictions_train': predictions_train,\n",
    "                                        'training_execution_time': training_execution_time,\n",
    "                                        'prediction_execution_time': prediction_execution_time\n",
    "                                       }\n",
    "    \n",
    "    return model_and_predictions_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k_day\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "Detailed in [Chapter 4, Precision_top_K_Metrics](Precision_Top_K_Metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_day(df_day,top_k):\n",
    "    \n",
    "    # This takes the max of the predictions AND the max of label TX_FRAUD for each CUSTOMER_ID, \n",
    "    # and sorts by decreasing order of fraudulent prediction\n",
    "    df_day = df_day.groupby('CUSTOMER_ID').max().sort_values(by=\"predictions\", ascending=False).reset_index(drop=False)\n",
    "            \n",
    "    # Get the top k most suspicious cards\n",
    "    df_day_top_k=df_day.head(top_k)\n",
    "    list_detected_frauded_cards=list(df_day_top_k[df_day_top_k.TX_FRAUD==1].CUSTOMER_ID)\n",
    "    \n",
    "    # Compute precision top k\n",
    "    card_precision_top_k = len(list_detected_frauded_cards) / top_k\n",
    "    \n",
    "    return list_detected_frauded_cards, card_precision_top_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS).\n",
    "Detailed in [Chapter 4, Precision_top_K_Metrics](Precision_Top_K_Metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k(predictions_df, top_k, remove_detected_frauded_cards=True):\n",
    "\n",
    "    # Sort days by increasing order\n",
    "    list_days=list(predictions_df['TX_TIME_DAYS'].unique())\n",
    "    list_days.sort()\n",
    "    \n",
    "    # At first, the list of detected frauded cards is empty\n",
    "    list_detected_frauded_cards = []\n",
    "    \n",
    "    card_precision_top_k_per_day_list = []\n",
    "    nb_frauded_cards_per_day = []\n",
    "    \n",
    "    # For each day, compute precision top k\n",
    "    for day in list_days:\n",
    "        \n",
    "        df_day = predictions_df[predictions_df['TX_TIME_DAYS']==day]\n",
    "        df_day = df_day[['predictions', 'CUSTOMER_ID', 'TX_FRAUD']]\n",
    "        \n",
    "        # Let us remove detected frauded cards from the set of daily transactions\n",
    "        df_day = df_day[df_day.CUSTOMER_ID.isin(list_detected_frauded_cards)==False]\n",
    "        \n",
    "        nb_frauded_cards_per_day.append(len(df_day[df_day.TX_FRAUD==1].CUSTOMER_ID.unique()))\n",
    "        \n",
    "        detected_frauded_cards, card_precision_top_k = card_precision_top_k_day(df_day,top_k)\n",
    "        \n",
    "        card_precision_top_k_per_day_list.append(card_precision_top_k)\n",
    "        \n",
    "        # Let us update the list of detected frauded cards\n",
    "        if remove_detected_frauded_cards:\n",
    "            list_detected_frauded_cards.extend(detected_frauded_cards)\n",
    "        \n",
    "    # Compute the mean\n",
    "    mean_card_precision_top_k = np.array(card_precision_top_k_per_day_list).mean()\n",
    "    \n",
    "    # Returns precision top k per day as a list, and resulting mean\n",
    "    return nb_frauded_cards_per_day,card_precision_top_k_per_day_list,mean_card_precision_top_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_precision_top_k_custom\n",
    "\n",
    "First use in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_precision_top_k_custom(y_true, y_pred, top_k, transactions_df):\n",
    "    \n",
    "    # Let us create a predictions_df DataFrame, that contains all transactions matching the indices of the current fold\n",
    "    # (indices of the y_true vector)\n",
    "    predictions_df=transactions_df.iloc[y_true.index.values].copy()\n",
    "    predictions_df['predictions']=y_pred\n",
    "    \n",
    "    # Compute the CP@k using the function implemented in Chapter 4, Section 4.2\n",
    "    nb_frauded_cards_per_day,card_precision_top_k_per_day_list,mean_card_precision_top_k=\\\n",
    "        card_precision_top_k(predictions_df, top_k)\n",
    "    \n",
    "    # Return the mean_card_precision_top_k\n",
    "    return mean_card_precision_top_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance_assessment\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_assessment(predictions_df, output_feature='TX_FRAUD', \n",
    "                           prediction_feature='predictions', top_k_list=[100],\n",
    "                           rounded=True):\n",
    "    \n",
    "    AUC_ROC = metrics.roc_auc_score(predictions_df[output_feature], predictions_df[prediction_feature])\n",
    "    AP = metrics.average_precision_score(predictions_df[output_feature], predictions_df[prediction_feature])\n",
    "    \n",
    "    performances = pd.DataFrame([[AUC_ROC, AP]], \n",
    "                           columns=['AUC ROC','Average precision'])\n",
    "    \n",
    "    for top_k in top_k_list:\n",
    "    \n",
    "        _, _, mean_card_precision_top_k = card_precision_top_k(predictions_df, top_k)\n",
    "        performances['Card Precision@'+str(top_k)]=mean_card_precision_top_k\n",
    "        \n",
    "    if rounded:\n",
    "        performances = performances.round(3)\n",
    "    \n",
    "    return performances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance_assessment_model_collection\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_assessment_model_collection(fitted_models_and_predictions_dictionary, \n",
    "                                            transactions_df, \n",
    "                                            type_set='test',\n",
    "                                            top_k_list=[100]):\n",
    "\n",
    "    performances=pd.DataFrame() \n",
    "    \n",
    "    for classifier_name, model_and_predictions in fitted_models_and_predictions_dictionary.items():\n",
    "    \n",
    "        predictions_df=transactions_df\n",
    "            \n",
    "        predictions_df['predictions']=model_and_predictions['predictions_'+type_set]\n",
    "        \n",
    "        performances_model=performance_assessment(predictions_df, output_feature='TX_FRAUD', \n",
    "                                                   prediction_feature='predictions', top_k_list=top_k_list)\n",
    "        performances_model.index=[classifier_name]\n",
    "        \n",
    "        performances=performances.append(performances_model)\n",
    "        \n",
    "    return performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execution_times_model_collection\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_times_model_collection(fitted_models_and_predictions_dictionary):\n",
    "\n",
    "    execution_times=pd.DataFrame() \n",
    "    \n",
    "    for classifier_name, model_and_predictions in fitted_models_and_predictions_dictionary.items():\n",
    "    \n",
    "        execution_times_model=pd.DataFrame() \n",
    "        execution_times_model['Training execution time']=[model_and_predictions['training_execution_time']]\n",
    "        execution_times_model['Prediction execution time']=[model_and_predictions['prediction_execution_time']]\n",
    "        execution_times_model.index=[classifier_name]\n",
    "        \n",
    "        execution_times=execution_times.append(execution_times_model)\n",
    "        \n",
    "    return execution_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_class_from_fraud_probability\n",
    "\n",
    "First use in [Chapter 4, Threshold Based Metrics](Threshold_Based_Metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting classes from a vector of fraud probabilities and a threshold\n",
    "def get_class_from_fraud_probability(fraud_probabilities, threshold=0.5):\n",
    "    \n",
    "    predicted_classes = [0 if fraud_probability<threshold else 1 \n",
    "                         for fraud_probability in fraud_probabilities]\n",
    "\n",
    "    return predicted_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold_based_metrics\n",
    "\n",
    "First use in [Chapter 4, Threshold Based Metrics](Threshold_Based_Metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_based_metrics(fraud_probabilities, true_label, thresholds_list):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds_list:\n",
    "    \n",
    "        predicted_classes = get_class_from_fraud_probability(fraud_probabilities, threshold=threshold)\n",
    "    \n",
    "        (TN, FP, FN, TP) = metrics.confusion_matrix(true_label, predicted_classes).ravel()\n",
    "    \n",
    "        MME = (FP+FN)/(TN+FP+FN+TP)\n",
    "    \n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP)\n",
    "    \n",
    "        FPR = FP/(TN+FP)\n",
    "        FNR = FN/(TP+FN)\n",
    "        \n",
    "        BER = 1/2*(FPR+FNR)\n",
    "        \n",
    "        Gmean = np.sqrt(TPR*TNR)\n",
    "    \n",
    "        precision = 1 # 1 if TP+FP=0\n",
    "        FDR = 1 # 1 if TP+FP=0\n",
    "        \n",
    "        if TP+FP>0:\n",
    "            precision = TP/(TP+FP)\n",
    "            FDR=FP/(TP+FP)\n",
    "        \n",
    "        NPV = 1 # 1 if TN+FN=0\n",
    "        FOR = 1 # 1 if TN+FN=0\n",
    "        \n",
    "        if TN+FN>0:\n",
    "            NPV = TN/(TN+FN)\n",
    "            FOR = FN/(TN+FN)\n",
    "            \n",
    "        \n",
    "        F1_score = 2*(precision*TPR)/(precision+TPR)\n",
    "    \n",
    "        results.append([threshold, MME, TPR, TNR, FPR, FNR, BER, Gmean, precision, NPV, FDR, FOR, F1_score])\n",
    "        \n",
    "    results_df = pd.DataFrame(results,columns=['Threshold' ,'MME', 'TPR', 'TNR', 'FPR', 'FNR', 'BER', 'G-mean', 'Precision', 'NPV', 'FDR', 'FOR', 'F1 Score'])\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_summary_performances\n",
    "\n",
    "First use in [Chapter 5, Model Selection](Model_Selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_performances(performances_df, parameter_column_name=\"Parameters summary\"):\n",
    "\n",
    "    metrics = ['AUC ROC','Average precision','Card Precision@100']\n",
    "    performances_results=pd.DataFrame(columns=metrics)\n",
    "    \n",
    "    performances_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    best_estimated_parameters = []\n",
    "    validation_performance = []\n",
    "    test_performance = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "    \n",
    "        index_best_validation_performance = performances_df.index[np.argmax(performances_df[metric+' Validation'].values)]\n",
    "    \n",
    "        best_estimated_parameters.append(performances_df[parameter_column_name].iloc[index_best_validation_performance])\n",
    "        \n",
    "        validation_performance.append(\n",
    "                str(round(performances_df[metric+' Validation'].iloc[index_best_validation_performance],3))+\n",
    "                '+/-'+\n",
    "                str(round(performances_df[metric+' Validation'+' Std'].iloc[index_best_validation_performance],2))\n",
    "        )\n",
    "        \n",
    "        test_performance.append(\n",
    "                str(round(performances_df[metric+' Test'].iloc[index_best_validation_performance],3))+\n",
    "                '+/-'+\n",
    "                str(round(performances_df[metric+' Test'+' Std'].iloc[index_best_validation_performance],2))\n",
    "        )\n",
    "    \n",
    "    performances_results.loc[\"Best estimated parameters ($k^*$)\"]=best_estimated_parameters\n",
    "    performances_results.loc[\"Validation performance\"]=validation_performance\n",
    "    performances_results.loc[\"Test performance\"]=test_performance\n",
    "\n",
    "    optimal_test_performance = []\n",
    "    optimal_parameters = []\n",
    "\n",
    "    for metric in ['AUC ROC Test','Average precision Test','Card Precision@100 Test']:\n",
    "    \n",
    "        index_optimal_test_performance = performances_df.index[np.argmax(performances_df[metric].values)]\n",
    "    \n",
    "        optimal_parameters.append(performances_df[parameter_column_name].iloc[index_optimal_test_performance])\n",
    "    \n",
    "        optimal_test_performance.append(\n",
    "                str(round(performances_df[metric].iloc[index_optimal_test_performance],3))+\n",
    "                '+/-'+\n",
    "                str(round(performances_df[metric+' Std'].iloc[index_optimal_test_performance],2))\n",
    "        )\n",
    "\n",
    "    performances_results.loc[\"Optimal parameter(s)\"]=optimal_parameters\n",
    "    performances_results.loc[\"Optimal test performance\"]=optimal_test_performance\n",
    "    \n",
    "    return performances_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prequential_grid_search\n",
    "\n",
    "First use in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequential_grid_search(transactions_df, \n",
    "                            classifier, \n",
    "                            input_features, output_feature, \n",
    "                            parameters, scoring, \n",
    "                            start_date_training, \n",
    "                            n_folds=4,\n",
    "                            expe_type='Test',\n",
    "                            delta_train=7, \n",
    "                            delta_delay=7, \n",
    "                            delta_assessment=7,\n",
    "                            performance_metrics_list_grid=['roc_auc'],\n",
    "                            performance_metrics_list=['AUC ROC'],\n",
    "                            n_jobs=-1):\n",
    "    \n",
    "    estimators = [('scaler', sklearn.preprocessing.StandardScaler()), ('clf', classifier)]\n",
    "    pipe = sklearn.pipeline.Pipeline(estimators)\n",
    "    \n",
    "    prequential_split_indices=prequentialSplit(transactions_df,\n",
    "                                               start_date_training=start_date_training, \n",
    "                                               n_folds=n_folds, \n",
    "                                               delta_train=delta_train, \n",
    "                                               delta_delay=delta_delay, \n",
    "                                               delta_assessment=delta_assessment)\n",
    "    \n",
    "    grid_search = sklearn.model_selection.GridSearchCV(pipe, parameters, scoring=scoring, cv=prequential_split_indices, refit=False, n_jobs=n_jobs)\n",
    "    \n",
    "    X=transactions_df[input_features]\n",
    "    y=transactions_df[output_feature]\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    performances_df=pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(performance_metrics_list_grid)):\n",
    "        performances_df[performance_metrics_list[i]+' '+expe_type]=grid_search.cv_results_['mean_test_'+performance_metrics_list_grid[i]]\n",
    "        performances_df[performance_metrics_list[i]+' '+expe_type+' Std']=grid_search.cv_results_['std_test_'+performance_metrics_list_grid[i]]\n",
    "\n",
    "    performances_df['Parameters']=grid_search.cv_results_['params']\n",
    "    performances_df['Execution time']=grid_search.cv_results_['mean_fit_time']\n",
    "    \n",
    "    return performances_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_selection_wrapper\n",
    "\n",
    "First use in [Chapter 5, Model Selection](Model_Selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_wrapper(transactions_df, \n",
    "                            classifier, \n",
    "                            input_features, output_feature,\n",
    "                            parameters, \n",
    "                            scoring, \n",
    "                            start_date_training_for_valid,\n",
    "                            start_date_training_for_test,\n",
    "                            n_folds=4,\n",
    "                            delta_train=7, \n",
    "                            delta_delay=7, \n",
    "                            delta_assessment=7,\n",
    "                            performance_metrics_list_grid=['roc_auc'],\n",
    "                            performance_metrics_list=['AUC ROC'],\n",
    "                            n_jobs=-1):\n",
    "\n",
    "    # Get performances on the validation set using prequential validation\n",
    "    performances_df_validation=prequential_grid_search(transactions_df, classifier, \n",
    "                            input_features, output_feature,\n",
    "                            parameters, scoring, \n",
    "                            start_date_training=start_date_training_for_valid,\n",
    "                            n_folds=n_folds,\n",
    "                            expe_type='Validation',\n",
    "                            delta_train=delta_train, \n",
    "                            delta_delay=delta_delay, \n",
    "                            delta_assessment=delta_assessment,\n",
    "                            performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "                            performance_metrics_list=performance_metrics_list,\n",
    "                            n_jobs=n_jobs)\n",
    "    \n",
    "    # Get performances on the test set using prequential validation\n",
    "    performances_df_test=prequential_grid_search(transactions_df, classifier, \n",
    "                            input_features, output_feature,\n",
    "                            parameters, scoring, \n",
    "                            start_date_training=start_date_training_for_test,\n",
    "                            n_folds=n_folds,\n",
    "                            expe_type='Test',\n",
    "                            delta_train=delta_train, \n",
    "                            delta_delay=delta_delay, \n",
    "                            delta_assessment=delta_assessment,\n",
    "                            performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "                            performance_metrics_list=performance_metrics_list,\n",
    "                            n_jobs=n_jobs)\n",
    "    \n",
    "    # Bind the two resulting DataFrames\n",
    "    performances_df_validation.drop(columns=['Parameters','Execution time'], inplace=True)\n",
    "    performances_df=pd.concat([performances_df_test,performances_df_validation],axis=1)\n",
    "\n",
    "    # And return as a single DataFrame\n",
    "    return performances_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tx_stats\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of transactions per day, fraudulent transactions per day and fraudulent cards per day\n",
    "\n",
    "def get_tx_stats(transactions_df, start_date_df=\"2018-04-01\"):\n",
    "    \n",
    "    #Number of transactions per day\n",
    "    nb_tx_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['CUSTOMER_ID'].count()\n",
    "    #Number of fraudulent transactions per day\n",
    "    nb_fraudulent_transactions_per_day=transactions_df.groupby(['TX_TIME_DAYS'])['TX_FRAUD'].sum()\n",
    "    #Number of fraudulent cards per day\n",
    "    nb_frauded_card_per_day=transactions_df[transactions_df['TX_FRAUD']==1].groupby(['TX_TIME_DAYS']).CUSTOMER_ID.nunique()\n",
    "    \n",
    "    tx_stats=pd.DataFrame({\"nb_tx_per_day\":nb_tx_per_day,\n",
    "                           \"nb_fraudulent_transactions_per_day\":nb_fraudulent_transactions_per_day,\n",
    "                           \"nb_frauded_cards_per_day\":nb_frauded_card_per_day})\n",
    "\n",
    "    tx_stats=tx_stats.reset_index()\n",
    "    \n",
    "    start_date = datetime.datetime.strptime(start_date_df, \"%Y-%m-%d\")\n",
    "    tx_date=start_date+tx_stats['TX_TIME_DAYS'].apply(datetime.timedelta)\n",
    "    \n",
    "    tx_stats['tx_date']=tx_date\n",
    "    \n",
    "    return tx_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_template_tx_stats\n",
    "\n",
    "First use in [Chapter 3, Baseline Fraud Detection System](Baseline_FDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of transactions per day, fraudulent transactions per day and fraudulent cards per day\n",
    "\n",
    "def get_template_tx_stats(ax ,fs,\n",
    "                          start_date_training,\n",
    "                          title='',\n",
    "                          delta_train=7,\n",
    "                          delta_delay=7,\n",
    "                          delta_test=7,\n",
    "                          ylim=300):\n",
    "    \n",
    "    ax.set_title(title, fontsize=fs*1.5)\n",
    "    ax.set_ylim([0, ylim])\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=fs)\n",
    "    ax.set_ylabel('Number', fontsize=fs)\n",
    "    \n",
    "    plt.yticks(fontsize=fs*0.7) \n",
    "    plt.xticks(fontsize=fs*0.7)    \n",
    "\n",
    "    ax.axvline(start_date_training+datetime.timedelta(days=delta_train), 0,ylim, color=\"black\")\n",
    "    ax.axvline(start_date_test, 0, ylim, color=\"black\")\n",
    "    \n",
    "    ax.text(start_date_training+datetime.timedelta(days=2), ylim-20,'Training period', fontsize=fs)\n",
    "    ax.text(start_date_training+datetime.timedelta(days=delta_train+2), ylim-20,'Delay period', fontsize=fs)\n",
    "    ax.text(start_date_training+datetime.timedelta(days=delta_train+delta_delay+2), ylim-20,'Test period', fontsize=fs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_template_roc_curve\n",
    "\n",
    "First use in [Chapter 4, Threshold Free Metrics](Threshold_Free_Metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_roc_curve(ax, title,fs,random=True):\n",
    "    \n",
    "    ax.set_title(title, fontsize=fs)\n",
    "    ax.set_xlim([-0.01, 1.01])\n",
    "    ax.set_ylim([-0.01, 1.01])\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=fs)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=fs)\n",
    "    \n",
    "    if random:\n",
    "        ax.plot([0, 1], [0, 1],'r--',label=\"AUC ROC Random = 0.5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_template_pr_curve\n",
    "\n",
    "First use in [Chapter 4, Threshold Free Metrics](Threshold_Free_Metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_pr_curve(ax, title,fs, baseline=0.5):\n",
    "    ax.set_title(title, fontsize=fs)\n",
    "    ax.set_xlim([-0.01, 1.01])\n",
    "    ax.set_ylim([-0.01, 1.01])\n",
    "    \n",
    "    ax.set_xlabel('Recall (True Positive Rate)', fontsize=fs)\n",
    "    ax.set_ylabel('Precision', fontsize=fs)\n",
    "    \n",
    "    ax.plot([0, 1], [baseline, baseline],'r--',label='AP Random = {0:0.3f}'.format(baseline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_performance_plot\n",
    "\n",
    "First use in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance plot for a single performance metric\n",
    "def get_performance_plot(performances_df, \n",
    "                         ax, \n",
    "                         performance_metric, \n",
    "                         expe_type_list=['Test','Train'], \n",
    "                         expe_type_color_list=['#008000','#2F4D7E'],\n",
    "                         parameter_name=\"Tree maximum depth\",\n",
    "                         summary_performances=None):\n",
    "    \n",
    "    # expe_type_list is the list of type of experiments, typically containing 'Test', 'Train', or 'Valid'\n",
    "    # For all types of experiments\n",
    "    for i in range(len(expe_type_list)):\n",
    "    \n",
    "        # Column in performances_df for which to retrieve the data \n",
    "        performance_metric_expe_type=performance_metric+' '+expe_type_list[i]\n",
    "    \n",
    "        # Plot data on graph\n",
    "        ax.plot(performances_df['Parameters summary'], performances_df[performance_metric_expe_type], \n",
    "                color=expe_type_color_list[i], label = expe_type_list[i])\n",
    "        \n",
    "        # If performances_df contains confidence intervals, add them to the graph\n",
    "        if performance_metric_expe_type+' Std' in performances_df.columns:\n",
    "        \n",
    "            conf_min = performances_df[performance_metric_expe_type]\\\n",
    "                        -2*performances_df[performance_metric_expe_type+' Std']\n",
    "            conf_max = performances_df[performance_metric_expe_type]\\\n",
    "                        +2*performances_df[performance_metric_expe_type+' Std']\n",
    "    \n",
    "            ax.fill_between(performances_df['Parameters summary'], conf_min, conf_max, color=expe_type_color_list[i], alpha=.1)\n",
    "\n",
    "    # If summary_performances table is present, adds vertical dashed bar for best estimated parameter \n",
    "    if summary_performances is not None:\n",
    "        best_estimated_parameter=summary_performances[performance_metric][['Best estimated parameters']].values[0]\n",
    "        best_estimated_performance=float(summary_performances[performance_metric][['Validation performance']].values[0].split(\"+/-\")[0])\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        ax.vlines(best_estimated_parameter, ymin, best_estimated_performance,\n",
    "                  linestyles=\"dashed\")\n",
    "    \n",
    "    # Set title, and x and y axes labels\n",
    "    ax.set_title(performance_metric+'\\n', fontsize=14)\n",
    "    ax.set(xlabel = parameter_name, ylabel=performance_metric)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_performances_plots\n",
    "\n",
    "First use in [Chapter 5, Validation Strategies](Validation_Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance plots for a set of performance metric\n",
    "def get_performances_plots(performances_df, \n",
    "                           performance_metrics_list=['AUC ROC', 'Average precision', 'Card Precision@100'], \n",
    "                           expe_type_list=['Test','Train'], expe_type_color_list=['#008000','#2F4D7E'],\n",
    "                           parameter_name=\"Tree maximum depth\",\n",
    "                           summary_performances=None):\n",
    "    \n",
    "    # Create as many graphs as there are performance metrics to display\n",
    "    n_performance_metrics = len(performance_metrics_list)\n",
    "    fig, ax = plt.subplots(1, n_performance_metrics, figsize=(5*n_performance_metrics,4))\n",
    "    \n",
    "    # Plot performance metric for each metric in performance_metrics_list\n",
    "    for i in range(n_performance_metrics):\n",
    "    \n",
    "        get_performance_plot(performances_df, ax[i], performance_metric=performance_metrics_list[i], \n",
    "                             expe_type_list=expe_type_list, \n",
    "                             expe_type_color_list=expe_type_color_list,\n",
    "                             parameter_name=parameter_name,\n",
    "                             summary_performances=summary_performances)\n",
    "    \n",
    "    ax[n_performance_metrics-1].legend(loc='upper left', \n",
    "                                       labels=expe_type_list, \n",
    "                                       bbox_to_anchor=(1.05, 1),\n",
    "                                       title=\"Type set\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5, \n",
    "                        hspace=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
