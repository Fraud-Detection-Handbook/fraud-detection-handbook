
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Sequential models and representation learning &#8212; Reproducible Machine Learning for Credit Card Fraud detection - Practical handbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/book.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/jtag_0.js"></script>
    <script src="../_static/jtag_1.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/SequentialModeling.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Real-world data" href="RealWorldData.html" />
    <link rel="prev" title="3. Autoencoders and anomaly detection" href="Autoencoders.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Reproducible Machine Learning for Credit Card Fraud detection - Practical handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Foreword.html">
   Foreword
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  1. Book overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_1_BookContent/BookContent.html">
   1. Book content and intended audience
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_1_BookContent/BookContributions.html">
   2. Book contributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_1_BookContent/HowToUse.html">
   3. How to use this book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  2. Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_2_Background/Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_2_Background/CreditCardFraud.html">
   2. Credit card fraud scenarios
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_2_Background/FDS.html">
   3. Credit card fraud detection system
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_2_Background/MachineLearningForFraudDetection.html">
   4. Machine learning for credit card fraud detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_2_Background/Summary.html">
   5. Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  3. Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_3_GettingStarted/Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_3_GettingStarted/SimulatedDataset.html">
   2. Transaction data simulator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_3_GettingStarted/BaselineFeatureTransformation.html">
   3. Baseline feature transformation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_3_GettingStarted/BaselineModeling.html">
   4. Baseline fraud detection system
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_3_GettingStarted/Baseline_RealWorldData.html">
   5. Real-world data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_3_GettingStarted/Summary.html">
   6. Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  4. Performance metrics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_4_PerformanceMetrics/Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_4_PerformanceMetrics/ThresholdBased.html">
   2. Threshold-based metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_4_PerformanceMetrics/ThresholdFree.html">
   3. Threshold-free metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_4_PerformanceMetrics/TopKBased.html">
   4. Precision top-k metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_4_PerformanceMetrics/Assessment_RealWorldData.html">
   5. Real-world data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_4_PerformanceMetrics/Summary.html">
   6. Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  5. Model validation and model selection
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_5_ModelValidationAndSelection/Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_5_ModelValidationAndSelection/ValidationStrategies.html">
   2. Validation strategies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_5_ModelValidationAndSelection/ModelSelection.html">
   3. Model selection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_5_ModelValidationAndSelection/ModelSelection_RealWorldData.html">
   4. Real-world data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_5_ModelValidationAndSelection/Summary.html">
   5. Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  6. Imbalanced learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_6_ImbalancedLearning/Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_6_ImbalancedLearning/CostSensitive.html">
   2. Cost-sensitive learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_6_ImbalancedLearning/Resampling.html">
   3. Resampling strategies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_6_ImbalancedLearning/Ensembling.html">
   4. Ensemble methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_6_ImbalancedLearning/Imbalanced_RealWorldData.html">
   5. Real-world data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_6_ImbalancedLearning/Summary.html">
   6. Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  7. Deep learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FeedForwardNeuralNetworks.html">
   2. Feed-forward neural network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Autoencoders.html">
   3. Autoencoders and anomaly detection
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Sequential models and representation learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RealWorldData.html">
   5. Real-world data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Summary.html">
   6. Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_References/shared_functions.html">
   1. Shared functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_References/bibliography.html">
   2. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Chapter_7_DeepLearning/SequentialModeling.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook/issues/new?title=Issue%20on%20page%20%2FChapter_7_DeepLearning/SequentialModeling.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook/edit/main/Chapter_7_DeepLearning/SequentialModeling.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Fraud-Detection-Handbook/fraud-detection-handbook/main?urlpath=tree/Chapter_7_DeepLearning/SequentialModeling.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/Fraud-Detection-Handbook/fraud-detection-handbook/blob/main/Chapter_7_DeepLearning/SequentialModeling.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#context-aware-fraud-detection">
   4.1. Context-aware fraud detection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expert-representations">
     4.1.1. Expert representations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#automatic-representations">
     4.1.2. Automatic representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-processing">
   4.2. Data processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setting-the-sequence-length">
     4.2.1. Setting the sequence length
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordering-elements-chronologically">
     4.2.2. Ordering elements chronologically
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#separating-data-according-to-the-landmark-variable-customer-id">
     4.2.3. Separating data according to the landmark variable (Customer ID)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#turning-a-customer-sequence-into-fixed-size-sequences">
     4.2.4. Turning a customer sequence into fixed size sequences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-the-sequences-of-transaction-features-on-the-fly-from-the-sequences-of-indices">
     4.2.5. Generating the sequences of transaction features on the fly from the sequences of indices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#efficient-implementation-with-pandas-and-groupby">
     4.2.6. Efficient implementation with pandas and groupby
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#managing-sequence-creation-into-a-torch-dataset">
     4.2.7. Managing sequence creation into a torch Dataset
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-neural-network-for-fraud-detection">
   4.3. Convolutional neural network for fraud detection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-convolutions">
     4.3.1. 1D-convolutions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stacking-convolutional-layers">
     4.3.2. Stacking convolutional layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-with-a-convolutional-neural-network">
     4.3.3. Classification with a convolutional neural network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     4.3.4. Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-1d-convolutional-neural-etwork">
     4.3.5. Training the 1D convolutional neural etwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation">
     4.3.6. Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#long-short-term-memory-network">
   4.4. Long Short-Term Memory network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lstm-for-fraud-detection">
     4.4.1. LSTM for fraud detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     4.4.2. Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-lstm">
     4.4.3. Training the LSTM
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     4.4.4. Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#towards-more-advanced-modeling-with-attention">
   4.5. Towards more advanced modeling with Attention
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attention-for-fraud-detection">
     4.5.1. Attention for fraud detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     4.5.2. Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-does-it-work">
     4.5.3. How does it work?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-lstm-with-attention">
     4.5.4. Training the LSTM with Attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#validation">
     4.5.5. Validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#seq-2-seq-autoencoders">
   4.6. Seq-2-Seq Autoencoders
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prequential-grid-search">
   4.7. Prequential grid search
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grid-search-on-the-1-d-convolutional-neural-network">
     4.7.1. Grid search on the 1-D Convolutional Neural Network
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grid-search-on-the-long-short-term-memory">
   4.8. Grid search on the Long Short Term Memory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grid-search-on-the-lstm-with-attention">
   4.9. Grid search on the LSTM with Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-of-results">
   4.10. Saving of results
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#benchmark-summary">
   4.11. Benchmark summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   4.12. Conclusion
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="sequential-models-and-representation-learning">
<span id="sequentialmodeling"></span><h1><span class="section-number">4. </span>Sequential models and representation learning<a class="headerlink" href="#sequential-models-and-representation-learning" title="Permalink to this headline">¶</a></h1>
<p>In credit card fraud detection, the choice of features is crucial for accurate classification. Considerable research effort has been dedicated to the development of features that are relevant to characterize fraudulent patterns. Feature engineering strategies based on feature aggregations have become commonly adopted <span id="id1">[<a class="reference internal" href="../Chapter_References/bibliography.html#id95">BASO16</a>]</span>. Feature aggregation establishes a connection between the current transaction and other related transactions by computing statistics over their variables. They often originate from complex rules defined from human expertise, and many experiments report that they significantly improve detection <span id="id2">[<a class="reference internal" href="../Chapter_References/bibliography.html#id6">DPCLB+14</a>, <a class="reference internal" href="../Chapter_References/bibliography.html#id98">DJS+20</a>, <a class="reference internal" href="../Chapter_References/bibliography.html#id97">JGZ+18</a>]</span>.</p>
<p>Using feature aggregation for fraud detection is part of a research topic referred to as “context-aware fraud detection”, where one considers the context (e.g. the cardholder history) associated with a transaction to make the decision. This allows, for instance, to give insights into the properties of the transaction relative to the usual purchase patterns of the cardholder and/or terminal, which is intuitively a relevant piece of information.</p>
<div class="section" id="context-aware-fraud-detection">
<h2><span class="section-number">4.1. </span>Context-aware fraud detection<a class="headerlink" href="#context-aware-fraud-detection" title="Permalink to this headline">¶</a></h2>
<p>The context is established based on a landmark variable, which is in most of the cases the Customer ID. Concretely, one starts by building the sequence of historical transactions, chronologically ordered from the oldest to the current one, that have the same value for the landmark variable as the current transaction.</p>
<p><img alt="Building the transaction sequence" src="../_images/transaction_sequence.png" /></p>
<p>This whole sequence is the raw basis for context-aware approaches. Indeed, the general process relies on the construction of new features or representations for each given transaction based on its contextual sequence. However, approaches can be divided into two broad categories, the first (Expert representations) relying on domain knowledge from experts to create rules and build feature aggregation, and the second (Automatic representations) oriented towards automated feature extraction strategies with deep learning models.</p>
<div class="section" id="expert-representations">
<h3><span class="section-number">4.1.1. </span>Expert representations<a class="headerlink" href="#expert-representations" title="Permalink to this headline">¶</a></h3>
<p>To build expert features from the base sequence, a selection layer relying on expert constraints (same Merchant Category code, same Country, more recent than a week, etc.) is first applied to obtain a more specific subsequence. Then, an aggregation function (sum, avg, …) is computed over the values of a chosen feature (e.g. amount) in the subsequence. For more details, <span id="id3">[<a class="reference internal" href="../Chapter_References/bibliography.html#id95">BASO16</a>]</span> provides a formal definition of such feature aggregations. Expert features are not necessarily transferable in any fraud detection domain since they rely on specific features that might not always be available. Nevertheless, in practice, the landmark features, constraints features, and aggregated features are often chosen from a set of frequent attributes, comprising Time, Amount, Country, Merchant Category, Customer ID, transaction type.</p>
<p><img alt="Expert features creation" src="../_images/expert_agg.png" /></p>
</div>
<div class="section" id="automatic-representations">
<h3><span class="section-number">4.1.2. </span>Automatic representations<a class="headerlink" href="#automatic-representations" title="Permalink to this headline">¶</a></h3>
<p>The other family of context-aware approaches considers the sequence directly as input in a model and lets it automatically learn the right connections to optimize fraud detection. The advantage of not relying on human knowledge to build the relevant features is obviously to save the costly resources and to ease adaptability and maintenance. Moreover, models can be pushed towards large architectures and learn very complex variable relationships automatically from data. However, it requires a sufficiently large dataset to properly identify the relevant patterns. Otherwise, the feature representations are not very accurate or useful.</p>
<p>A baseline technique for automatic usage of contextual data would be to exploit the transaction sequence in its entirety (e.g. flattening it into a set of features), but this removes the information about the order and could lead to a very high dimensional feature space. A more popular strategy consists of (1) turning the sequences into fixed-size sequences and (2) using a special family of models that are able to deal with sequences naturally and summarizing them into relevant vector representations for fraud classification.</p>
<p>This strategy is often referred to as sequential learning, the study of learning algorithms for sequential data. The sequential dependency between data points is learned at the algorithmic level. This includes sliding window methods, which often tend to ignore the order between data points within the window, but also models that are designed explicitly to consider the sequential order between consecutive data points (e.g. a Markov Chain). Such models can be found in the family of deep learning architectures under the <em>recurrent neural networks</em> category. The link between consecutive elements of the sequence is embedded in the design of the recurrent architecture, where the computation of the hidden state/layer of a more recent event depends on hidden states of previous events.</p>
<p><img alt="Illustration of a recurrent model" src="../_images/recurrent_model.png" /></p>
<p>Sliding window methods include architectures like 1D <em>convolutional neural networks</em> (1D-CNN), and sequential methods include architectures like the <em>long short-term memory</em> (LSTM). Such architectures have proven to be very efficient in the context of fraud detection in the past <span id="id4">[<a class="reference internal" href="../Chapter_References/bibliography.html#id97">JGZ+18</a>]</span>.</p>
<p>In this section, the goal is to explore automatic representation learning from cardholders’ sequences of transactions and its application in context-aware fraud detection. The content starts with the practical aspects of building the pipeline to manage sequential data. Then, three architectures are successively explored: a 1D-CNN, an LSTM, and a more complex model that uses an LSTM with <em>Attention</em> <span id="id5">[<a class="reference internal" href="../Chapter_References/bibliography.html#id114">BCB14</a>]</span>. The section concludes on perspectives of other modeling possibilities such as sequence-to-sequence autoencoders <span id="id6">[<a class="reference internal" href="../Chapter_References/bibliography.html#id113">AHJ+20</a>, <a class="reference internal" href="../Chapter_References/bibliography.html#id115">SVL14</a>]</span>, or other combinations of the explored models.</p>
</div>
</div>
<div class="section" id="data-processing">
<h2><span class="section-number">4.2. </span>Data processing<a class="headerlink" href="#data-processing" title="Permalink to this headline">¶</a></h2>
<p>With sequence modeling, the building of the data processing pipeline has special importance. In particular, as explained in the introduction, its role is to create the input sequences for the sequential models to learn the representations.</p>
<p>The most popular landmark variable to establish the sequence is the Customer ID. Indeed, by the very definition of credit card fraud being a payment done by someone other than the cardholder, it makes the most sense to look at the history of the customer in order to determine when a card payment is fraudulent.</p>
<p>Therefore, the goal here is to establish <code class="docutils literal notranslate"><span class="pre">Datasets</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoaders</span></code> that provide, given a transaction index in the dataset, the sequence of previous transactions (including the one referred by the index) from the same cardholder. Moreover, since models usually deal with fixed-sized sequences, the sequence length will be a parameter, and sequences that are too long (resp. too short) will be cut (resp. padded).</p>
<p>As usual, let us start by loading a fixed training and validation period from the processed dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialization: Load shared functions and simulated data </span>

<span class="c1"># Load shared functions</span>
<span class="o">!</span>curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/Chapter_References/shared_functions.py
<span class="o">%</span><span class="k">run</span> shared_functions.py

<span class="c1"># Get simulated data from Github repository</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;simulated-data-transformed&quot;</span><span class="p">):</span>
    <span class="o">!</span>git clone https://github.com/Fraud-Detection-Handbook/simulated-data-transformed
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 63257  100 63257    0     0   537k      0 --:--:-- --:--:-- --:--:--  532k
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">DIR_INPUT</span><span class="o">=</span><span class="s1">&#39;simulated-data-transformed/data/&#39;</span> 

<span class="n">BEGIN_DATE</span> <span class="o">=</span> <span class="s2">&quot;2018-06-11&quot;</span>
<span class="n">END_DATE</span> <span class="o">=</span> <span class="s2">&quot;2018-09-14&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Load  files&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">time</span> transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0}</span><span class="s2"> transactions loaded, containing </span><span class="si">{1}</span><span class="s2"> fraudulent transactions&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transactions_df</span><span class="p">),</span><span class="n">transactions_df</span><span class="o">.</span><span class="n">TX_FRAUD</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>

<span class="n">output_feature</span><span class="o">=</span><span class="s2">&quot;TX_FRAUD&quot;</span>

<span class="n">input_features</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;TX_AMOUNT&#39;</span><span class="p">,</span><span class="s1">&#39;TX_DURING_WEEKEND&#39;</span><span class="p">,</span> <span class="s1">&#39;TX_DURING_NIGHT&#39;</span><span class="p">,</span> <span class="s1">&#39;CUSTOMER_ID_NB_TX_1DAY_WINDOW&#39;</span><span class="p">,</span>
       <span class="s1">&#39;CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW&#39;</span><span class="p">,</span> <span class="s1">&#39;CUSTOMER_ID_NB_TX_7DAY_WINDOW&#39;</span><span class="p">,</span>
       <span class="s1">&#39;CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW&#39;</span><span class="p">,</span> <span class="s1">&#39;CUSTOMER_ID_NB_TX_30DAY_WINDOW&#39;</span><span class="p">,</span>
       <span class="s1">&#39;CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW&#39;</span><span class="p">,</span> <span class="s1">&#39;TERMINAL_ID_NB_TX_1DAY_WINDOW&#39;</span><span class="p">,</span>
       <span class="s1">&#39;TERMINAL_ID_RISK_1DAY_WINDOW&#39;</span><span class="p">,</span> <span class="s1">&#39;TERMINAL_ID_NB_TX_7DAY_WINDOW&#39;</span><span class="p">,</span>
       <span class="s1">&#39;TERMINAL_ID_RISK_7DAY_WINDOW&#39;</span><span class="p">,</span> <span class="s1">&#39;TERMINAL_ID_NB_TX_30DAY_WINDOW&#39;</span><span class="p">,</span>
       <span class="s1">&#39;TERMINAL_ID_RISK_30DAY_WINDOW&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Load  files
CPU times: user 311 ms, sys: 237 ms, total: 548 ms
Wall time: 570 ms
919767 transactions loaded, containing 8195 fraudulent transactions
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the starting day for the training period, and the deltas</span>
<span class="n">start_date_training</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="s2">&quot;2018-07-25&quot;</span><span class="p">,</span> <span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">delta_train</span><span class="o">=</span><span class="mi">7</span>
<span class="n">delta_delay</span><span class="o">=</span><span class="mi">7</span>
<span class="n">delta_test</span><span class="o">=</span><span class="mi">7</span>


<span class="n">delta_valid</span> <span class="o">=</span> <span class="n">delta_test</span>

<span class="n">start_date_training_with_valid</span> <span class="o">=</span> <span class="n">start_date_training</span><span class="o">+</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=-</span><span class="p">(</span><span class="n">delta_delay</span><span class="o">+</span><span class="n">delta_valid</span><span class="p">))</span>

<span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">valid_df</span><span class="p">)</span><span class="o">=</span><span class="n">get_train_test_set</span><span class="p">(</span><span class="n">transactions_df</span><span class="p">,</span><span class="n">start_date_training_with_valid</span><span class="p">,</span>
                                       <span class="n">delta_train</span><span class="o">=</span><span class="n">delta_train</span><span class="p">,</span><span class="n">delta_delay</span><span class="o">=</span><span class="n">delta_delay</span><span class="p">,</span><span class="n">delta_test</span><span class="o">=</span><span class="n">delta_test</span><span class="p">)</span>

<span class="c1"># By default, scales input data</span>
<span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">valid_df</span><span class="p">)</span><span class="o">=</span><span class="n">scaleData</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">valid_df</span><span class="p">,</span><span class="n">input_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;TX_AMOUNT&#39;,
 &#39;TX_DURING_WEEKEND&#39;,
 &#39;TX_DURING_NIGHT&#39;,
 &#39;CUSTOMER_ID_NB_TX_1DAY_WINDOW&#39;,
 &#39;CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW&#39;,
 &#39;CUSTOMER_ID_NB_TX_7DAY_WINDOW&#39;,
 &#39;CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW&#39;,
 &#39;CUSTOMER_ID_NB_TX_30DAY_WINDOW&#39;,
 &#39;CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW&#39;,
 &#39;TERMINAL_ID_NB_TX_1DAY_WINDOW&#39;,
 &#39;TERMINAL_ID_RISK_1DAY_WINDOW&#39;,
 &#39;TERMINAL_ID_NB_TX_7DAY_WINDOW&#39;,
 &#39;TERMINAL_ID_RISK_7DAY_WINDOW&#39;,
 &#39;TERMINAL_ID_NB_TX_30DAY_WINDOW&#39;,
 &#39;TERMINAL_ID_RISK_30DAY_WINDOW&#39;]
</pre></div>
</div>
</div>
</div>
<p>This time, additionally to the above features, building the sequences will require two additional fields from the DataFrames:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">CUSTOMER_ID</span></code>: the landmark variable that will be used to select past transactions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TX_DATETIME</span></code>: the time variable that will allow building sequences in chronological order.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dates</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">customer_ids</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<p>There are multiple ways to implement sequence creation for training/validation. One way is to precompute, for each transaction, the indices of the previous transactions of the sequence and to store them. Then, to build the sequences of features on the fly from the indices.</p>
<p>Here we propose some steps to proceed but keep in mind that other solutions are just as valid.</p>
<div class="section" id="setting-the-sequence-length">
<h3><span class="section-number">4.2.1. </span>Setting the sequence length<a class="headerlink" href="#setting-the-sequence-length" title="Permalink to this headline">¶</a></h3>
<p>The first step is to set a sequence length. In the literature, 5 or 10 are two values that are often chosen <span id="id7">[<a class="reference internal" href="../Chapter_References/bibliography.html#id97">JGZ+18</a>]</span>. This can later be a parameter to tune but here, let us arbitrarily set it to 5 to begin with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seq_len</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="ordering-elements-chronologically">
<h3><span class="section-number">4.2.2. </span>Ordering elements chronologically<a class="headerlink" href="#ordering-elements-chronologically" title="Permalink to this headline">¶</a></h3>
<p>In our case, transactions are already sorted, but in the most general case, to build the sequences, it is necessary to sort all transactions chronologically and keep the sorting indices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">indices_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dates</span><span class="p">)</span>
<span class="n">sorted_dates</span> <span class="o">=</span> <span class="n">dates</span><span class="p">[</span><span class="n">indices_sort</span><span class="p">]</span>
<span class="n">sorted_ids</span> <span class="o">=</span> <span class="n">customer_ids</span><span class="p">[</span><span class="n">indices_sort</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="separating-data-according-to-the-landmark-variable-customer-id">
<h3><span class="section-number">4.2.3. </span>Separating data according to the landmark variable (Customer ID)<a class="headerlink" href="#separating-data-according-to-the-landmark-variable-customer-id" title="Permalink to this headline">¶</a></h3>
<p>After sorting, the dataset is a large sequence with all transactions from all cardholders. We can separate it into several sequences that each contain only the transactions of a single cardholder. Finally, each customer sequence can be turned into several fixed-size sequences using a sliding window and padding.</p>
<p>To separate the dataset, let us get the list of customers:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unique_customer_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">sorted_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unique_customer_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</pre></div>
</div>
</div>
</div>
<p>For each customer, the associated subsequence can be selected with a boolean mask. Here is for example the sequence of transaction IDs for customer <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">current_customer_id</span> <span class="o">=</span> <span class="n">unique_customer_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">customer_mask</span> <span class="o">=</span> <span class="n">sorted_ids</span> <span class="o">==</span> <span class="n">current_customer_id</span>
<span class="c1"># this is the full sequence of transaction indices (after sort) for customer 0</span>
<span class="n">customer_full_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">customer_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># this is the full sequence of transaction indices (before sort) for customer 0</span>
<span class="n">customer_full_seq_original_indices</span> <span class="o">=</span> <span class="n">indices_sort</span><span class="p">[</span><span class="n">customer_full_seq</span><span class="p">]</span>
<span class="n">customer_full_seq_original_indices</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 1888, 10080, 12847, 15627, 18908, 22842, 37972, 42529, 44495,
       48980, 58692, 63977])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="turning-a-customer-sequence-into-fixed-size-sequences">
<h3><span class="section-number">4.2.4. </span>Turning a customer sequence into fixed size sequences<a class="headerlink" href="#turning-a-customer-sequence-into-fixed-size-sequences" title="Permalink to this headline">¶</a></h3>
<p>The above sequence is the whole sequence for customer <code class="docutils literal notranslate"><span class="pre">0</span></code>. But the goal is to have, for each transaction <code class="docutils literal notranslate"><span class="pre">i</span></code> of this sequence, a fixed size sequence that ends with transaction <code class="docutils literal notranslate"><span class="pre">i</span></code>, which will be used as input in the sequential model to predict the label of transaction <code class="docutils literal notranslate"><span class="pre">i</span></code>. In the example above:</p>
<ul class="simple">
<li><p>For transaction 1888: the 4 previous transactions are [none,none,none,none] (Note: none can be replaced with a default value like -1). So the sequence [none, none, none, none, 1888] will be created.</p></li>
<li><p>For transaction 10080, the sequence will be [none, none, none, 1888, 10080]</p></li>
<li><p>…</p></li>
<li><p>For transaction 37972, the sequence will be [12847, 15627, 18908, 22842, 37972]</p></li>
<li><p>Etc.</p></li>
</ul>
<p>Using a sliding window (or rolling window) allows to obtain those sequences:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rolling_window</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">window</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span><span class="o">*-</span><span class="mi">1</span><span class="p">,</span><span class="n">array</span><span class="p">])</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">strides</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">stride_tricks</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">customer_all_seqs</span> <span class="o">=</span> <span class="n">rolling_window</span><span class="p">(</span><span class="n">customer_full_seq_original_indices</span><span class="p">,</span><span class="n">seq_len</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">customer_all_seqs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[   -1,    -1,    -1,    -1,  1888],
       [   -1,    -1,    -1,  1888, 10080],
       [   -1,    -1,  1888, 10080, 12847],
       [   -1,  1888, 10080, 12847, 15627],
       [ 1888, 10080, 12847, 15627, 18908],
       [10080, 12847, 15627, 18908, 22842],
       [12847, 15627, 18908, 22842, 37972],
       [15627, 18908, 22842, 37972, 42529],
       [18908, 22842, 37972, 42529, 44495],
       [22842, 37972, 42529, 44495, 48980],
       [37972, 42529, 44495, 48980, 58692],
       [42529, 44495, 48980, 58692, 63977]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="generating-the-sequences-of-transaction-features-on-the-fly-from-the-sequences-of-indices">
<h3><span class="section-number">4.2.5. </span>Generating the sequences of transaction features on the fly from the sequences of indices<a class="headerlink" href="#generating-the-sequences-of-transaction-features-on-the-fly-from-the-sequences-of-indices" title="Permalink to this headline">¶</a></h3>
<p>From the indices’ sequences and the features of each transaction (available in <code class="docutils literal notranslate"><span class="pre">x_train</span></code>), building the features sequences is straightforward. Let us do it for the 6th sequence:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">customer_all_seqs</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([10080, 12847, 15627, 18908, 22842])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="n">input_features</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sixth_sequence</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">customer_all_seqs</span><span class="p">[</span><span class="mi">5</span><span class="p">],:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sixth_sequence</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.6965, -0.6306,  2.1808, -0.8466,  0.0336, -1.1665,  0.0176, -0.9341,
          0.2310, -0.9810, -0.0816, -0.3445, -0.1231, -0.2491, -0.1436],
        [ 0.0358, -0.6306, -0.4586, -0.8466,  0.4450, -1.1665,  0.1112, -0.8994,
          0.2278,  0.0028, -0.0816,  0.6425, -0.1231, -0.0082, -0.1436],
        [ 1.1437, -0.6306, -0.4586, -0.3003,  0.7595, -1.0352,  0.2462, -0.8994,
          0.2458,  1.9702, -0.0816,  1.3005, -0.1231,  1.7989, -0.1436],
        [ 0.3645, -0.6306, -0.4586,  0.2461,  0.6804, -1.0352,  0.3186, -0.8647,
          0.2514,  1.9702, -0.0816,  0.3135, -0.1231, -0.8514, -0.1436],
        [ 0.3348, -0.6306, -0.4586, -0.3003,  0.7462, -1.1665,  0.2494, -0.8994,
          0.2262, -0.9810, -0.0816, -2.3185, -0.1231, -1.5743, -0.1436]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sixth_sequence</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 15])
</pre></div>
</div>
</div>
</div>
<p>Note: Here, the sequence of indices (<code class="docutils literal notranslate"><span class="pre">customer_all_seqs[5]</span></code>) was made of valid indices. When there are invalid indices (<code class="docutils literal notranslate"><span class="pre">-1</span></code>), the idea is to put a “padding transaction” (e.g. with all features equal to zero or equal to the average value that they have in the training set) in the final sequence. To obtain a homogeneous code that can be used for both valid and invalid indices, on can append the “padding transaction” to <code class="docutils literal notranslate"><span class="pre">x_train</span></code> at the end and replace all <code class="docutils literal notranslate"><span class="pre">-1</span></code> with the index of this added transaction.</p>
</div>
<div class="section" id="efficient-implementation-with-pandas-and-groupby">
<h3><span class="section-number">4.2.6. </span>Efficient implementation with pandas and groupby<a class="headerlink" href="#efficient-implementation-with-pandas-and-groupby" title="Permalink to this headline">¶</a></h3>
<p>The above steps are described for educational purposes as they allow to understand all the necessary operations to build the sequences of transactions. In practice, because this process requires a time-consuming loop over all Customer IDs, it is better to rely on a dataframe instead and use the pandas <code class="docutils literal notranslate"><span class="pre">groupby</span></code> function. More precisely, the idea is to group the elements of the transaction dataframe by Customer ID and to use the <code class="docutils literal notranslate"><span class="pre">shift</span></code> function to determine, for each transaction, the ones that occurred before. In order not to edit the original dataframe, let us first create a new one that only contains the necessary features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_ids_dates</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">:</span> <span class="n">customer_ids</span><span class="p">,</span>
        <span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">:</span> <span class="n">dates</span><span class="p">})</span>

<span class="c1">#checking if the transaction are chronologically ordered</span>
<span class="n">datetime_diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_ids_dates</span><span class="p">[</span><span class="s2">&quot;TX_DATETIME&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_ids_dates</span><span class="p">[</span><span class="s2">&quot;TX_DATETIME&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">total_seconds</span><span class="p">()</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">datetime_diff</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now add a new column with the initial row indices, that will be later used with the <code class="docutils literal notranslate"><span class="pre">shift</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_ids_dates</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_ids_dates</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_ids_dates</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CUSTOMER_ID</th>
      <th>TX_DATETIME</th>
      <th>tmp_index</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>579</td>
      <td>2018-07-11 00:00:54</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>181</td>
      <td>2018-07-11 00:01:59</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4386</td>
      <td>2018-07-11 00:03:39</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4599</td>
      <td>2018-07-11 00:05:50</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4784</td>
      <td>2018-07-11 00:06:04</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The next step is to group the elements by Customer ID:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_groupby_customer_id</span> <span class="o">=</span> <span class="n">df_ids_dates</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;CUSTOMER_ID&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now it is possible to compute a shifted <code class="docutils literal notranslate"><span class="pre">tmp_index</span></code> with respect to the grouping by <code class="docutils literal notranslate"><span class="pre">CUSTOMER_ID</span></code>. For instance, shifting by 0 gives the current transaction index and shifting by 1 gives the previous transaction index (or NaN if the current transaction is the first transaction of the customer).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_groupby_customer_id</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0            0
1            1
2            2
3            3
4            4
         ...  
66923    66923
66924    66924
66925    66925
66926    66926
66927    66927
Name: tmp_index, Length: 66928, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_groupby_customer_id</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0            NaN
1            NaN
2            NaN
3            NaN
4            NaN
          ...   
66923    66805.0
66924    64441.0
66925    66777.0
66926    63338.0
66927    60393.0
Name: tmp_index, Length: 66928, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>To obtain the whole sequences of indices, the only thing to do is to loop over the shift parameter, from <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> - 1 to 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence_indices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;tx_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">):</span> <span class="n">df_groupby_customer_id</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">)</span>

<span class="n">sequence_indices</span> <span class="o">=</span> <span class="n">sequence_indices</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence_indices</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tx_0</th>
      <th>tx_1</th>
      <th>tx_2</th>
      <th>tx_3</th>
      <th>tx_4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>-1</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As a sanity check, let us see if this method computes the same sequences as the previous method for transaction 12847, 15627 and 18908 which were (see 4.2.4):</p>
<ul class="simple">
<li><p>[   -1,    -1,  1888, 10080, 12847]</p></li>
<li><p>[   -1,  1888, 10080, 12847, 15627]</p></li>
<li><p>[ 1888, 10080, 12847, 15627, 18908]</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sequence_indices</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">12847</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sequence_indices</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">15627</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sequence_indices</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">18908</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[   -1    -1  1888 10080 12847]
[   -1  1888 10080 12847 15627]
[ 1888 10080 12847 15627 18908]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="managing-sequence-creation-into-a-torch-dataset">
<h3><span class="section-number">4.2.7. </span>Managing sequence creation into a torch Dataset<a class="headerlink" href="#managing-sequence-creation-into-a-torch-dataset" title="Permalink to this headline">¶</a></h3>
<p>Now that the process is ready and tested, the final step is to implement it within a torch <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> to use it in a training loop. To simplify the usage, let us consider the “zeros” padding strategy as default. The precomputation of indices and the creation of the padding transaction (a transaction with all features to zero) will be done at initialization. Then, the <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> function will build the sequence of features on the fly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">DEVICE</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> 
<span class="k">else</span><span class="p">:</span>
    <span class="n">DEVICE</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected device is&quot;</span><span class="p">,</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Selected device is cuda
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudSequenceDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">customer_ids</span><span class="p">,</span> <span class="n">dates</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">padding_mode</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="s1">&#39;Initialization&#39;</span>
        
        <span class="c1"># x,y,customer_ids, and dates must have the same length</span>
        
        <span class="c1"># storing the features x in self.features and adding the &quot;padding&quot; transaction at the end</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)])</span>
        <span class="k">elif</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;zeros&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">shape</span><span class="p">)])</span>            
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;padding_mode must be &quot;mean&quot; or &quot;zeros&quot;&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">customer_ids</span> <span class="o">=</span> <span class="n">customer_ids</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dates</span> <span class="o">=</span> <span class="n">dates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span>
        
        <span class="c1">#===== computing sequences ids =====  </span>
        
        
        <span class="n">df_ids_dates</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">:</span><span class="n">customer_ids</span><span class="p">,</span>
        <span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">:</span><span class="n">dates</span><span class="p">})</span>
        
        <span class="n">df_ids_dates</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_ids_dates</span><span class="p">))</span>
        <span class="n">df_groupby_customer_id</span> <span class="o">=</span> <span class="n">df_ids_dates</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;CUSTOMER_ID&quot;</span><span class="p">)</span>
        <span class="n">sequence_indices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;tx_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">):</span> <span class="n">df_groupby_customer_id</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">)</span>
        
        <span class="c1">#replaces -1 (padding) with the index of the padding transaction (last index of self.features)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sequences_ids</span> <span class="o">=</span> <span class="n">sequence_indices</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>              


    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s1">&#39;Denotes the total number of samples&#39;</span>
        <span class="c1"># not len(self.features) because of the added padding transaction</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">customer_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="s1">&#39;Generates one sample of data&#39;</span>
        <span class="c1"># Select sample index</span>
        
        <span class="n">tx_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequences_ids</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
            <span class="c1">#transposing because the CNN considers the channel dimension before the sequence dimension</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">tx_ids</span><span class="p">,:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">tx_ids</span><span class="p">,:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As a sanity check, let us try the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> within a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="n">input_features</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">x_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">valid_df</span><span class="p">[</span><span class="n">input_features</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="n">output_feature</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">y_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">valid_df</span><span class="p">[</span><span class="n">output_feature</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">seed_everything</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
          <span class="s1">&#39;shuffle&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
          <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>


<span class="c1"># Generators</span>

<span class="n">training_set</span> <span class="o">=</span> <span class="n">FraudSequenceDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">)</span>
<span class="n">training_generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="o">**</span><span class="n">train_loader_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us see how the first training batch looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">training_generator</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_batch</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([64, 15, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_batch</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([64])
</pre></div>
</div>
</div>
</div>
<p>The shape of <code class="docutils literal notranslate"><span class="pre">x_batch</span></code> is (<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>= 64,<code class="docutils literal notranslate"><span class="pre">number</span> <span class="pre">of</span> <span class="pre">features</span></code>= 15, <code class="docutils literal notranslate"><span class="pre">seq_len</span></code>= 5) which is the expected input for a 1-D convolutional network or a recurrent model like an LSTM.</p>
</div>
</div>
<div class="section" id="convolutional-neural-network-for-fraud-detection">
<h2><span class="section-number">4.3. </span>Convolutional neural network for fraud detection<a class="headerlink" href="#convolutional-neural-network-for-fraud-detection" title="Permalink to this headline">¶</a></h2>
<p>Convolutional neural networks (CNN) are neural networks with specific convolutional layers that allow (1) detecting specific patterns or shapes in patches of input and (2) reducing spatial complexity when dealing with large inputs (e.g. an image with millions of pixels).</p>
<p>To do that, they replace the regular fully connected layer with a layer of convolutional filters that performs a convolution operation over the input neurons.</p>
<p><img alt="Illustration of a 2D-convolutional layer" src="../_images/convnet.png" /></p>
<p>A convolutional layer has <code class="docutils literal notranslate"><span class="pre">num_filters</span></code> filters, with weights of a chosen dimension. If we consider a 2D-convolutional layer, each filter has 2 dimensions (width and height). In the example above, we consider an 8x6 input and a 3x3 filter. The convolution operation consists in sliding the filter over the input from left to right and from up to bottom, and each time computing the weighted sum of the input patch using the filter weights and applying an activation function to obtain an output similar to a regular neuron. In the figure, we represent with dashed squares on the left the first two input patches that the filter goes through and on the right the two corresponding outputs. Here we considered a <code class="docutils literal notranslate"><span class="pre">stride</span></code> parameter of 1, which means that the filter slides by 1 input each time. There is no padding, so the filter does not slide outside of the input, therefore the result is a map of features of dimension (8-(3-1))x(6-(3-1)), i.e. 6x4. But one can apply padding (considering that values outside the input are zeros) so that the output feature map has the same dimension as the input.</p>
<p>The convolution operation can be seen as a filter scanning the input to identify a specific pattern. The information of the feature map can be aggregated into single information with a global pooling (average, max).</p>
<div class="section" id="d-convolutions">
<h3><span class="section-number">4.3.1. </span>1D-convolutions<a class="headerlink" href="#d-convolutions" title="Permalink to this headline">¶</a></h3>
<p>Note: Although not represented in the figure, if the input is an image, the input is in fact 3-dimensional (a 2D map of 3 features also called channels, namely RGB levels). The user only defines 2 dimensions for the filter (along the “sliding” directions) but the filters are in reality 3-dimensional as well, the last dimension matching the channel dimension.</p>
<p>2D-convolutions are only used to analyze inputs for which it makes sense to slide along 2 dimensions. In our case, to deal with transaction sequences, it only makes sense to slide along the sequence axis. Therefore, for fraud detection, we resort to 1D-convolutions and define a single filter dimension (with length equal to the number of consecutive sequence elements on which the filter looks for patterns).</p>
</div>
<div class="section" id="stacking-convolutional-layers">
<h3><span class="section-number">4.3.2. </span>Stacking convolutional layers<a class="headerlink" href="#stacking-convolutional-layers" title="Permalink to this headline">¶</a></h3>
<p>One can stack convolution layers just like fully connected layers. For instance, let us consider an input transaction sequence with 5 transactions and 15 features for each transaction. If one defines a convolutional neural network with a first 1D-convolutional layer with 100 filters of length 2 and a second convolutional layer with 50 filters of length 2. Without padding, the successive features map’s dimension will be the following:</p>
<ul class="simple">
<li><p>The input dimension is (5,15): 5 is the sequence length and 15 is the number of channels.</p></li>
<li><p>The output dimension of the first convolutional layer is (4,100): each filter of dimension (2,15) will output a 1D feature map with 5-(2-1) = 4 features.</p></li>
<li><p>The output dimension of the second convolutional layer is (3,50): each filter of dimension (2,100) will output a 1D feature map with 4-(2-1) = 3 features.</p></li>
</ul>
<p>With padding, we can make sure that the sequence length does not change and obtain the dimensions (5,100) and (5,50) instead of (4,100) and (3,50).</p>
</div>
<div class="section" id="classification-with-a-convolutional-neural-network">
<h3><span class="section-number">4.3.3. </span>Classification with a convolutional neural network<a class="headerlink" href="#classification-with-a-convolutional-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Convolutional layers produce high-level features that detect the presence of patterns or combinations of patterns within the input. These features can be considered as automatic feature aggregates and can then be used in a final regular fully connected layer for classification, after a flattening operation. This operation can be done using a pooling operator along the sequence dimension or with a flattening operator that simply concatenates the channels of all elements of the sequence into a single global vector.</p>
</div>
<div class="section" id="implementation">
<h3><span class="section-number">4.3.4. </span>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h3>
<p>Convolutional layers are defined like regular layers. Instead of using the <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> module, one uses the specific layers for convolutional neural networks:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.Conv1d</span></code>: this module defines a convolutional layer. The parameters are the number of input channels, the number of filters, and the dimension of the filters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.ConstantPad1d</span></code>: this module allows us to pad a sequence with a constant value (e.g. 0) to obtain the desired sequence length after the subsequent convolutional layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.MaxPool1d</span></code> or <code class="docutils literal notranslate"><span class="pre">AvgPool1d</span></code>: these modules perform a pooling operation over the sequence dimension.</p></li>
</ul>
<p>Let us define a <code class="docutils literal notranslate"><span class="pre">FraudConvNet</span></code> module that makes use of the above <code class="docutils literal notranslate"><span class="pre">torch</span></code> modules to take as input a sequence of <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> transactions with <code class="docutils literal notranslate"><span class="pre">len(input_features)</span></code> features and predict if the last transaction is fraudulent. We will consider 2 padded convolutional layers, a max-pooling layer, a hidden fully connected layer, and an output fully connected layer with 1 output neuron.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudConvNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                     <span class="n">num_features</span><span class="p">,</span> 
                     <span class="n">seq_len</span><span class="p">,</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                     <span class="n">conv1_params</span> <span class="o">=</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> 
                     <span class="n">conv2_params</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                     <span class="n">max_pooling</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
            
            <span class="nb">super</span><span class="p">(</span><span class="n">FraudConvNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            
            <span class="c1"># parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            
            <span class="c1"># representation learning part</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv1_num_filters</span>  <span class="o">=</span> <span class="n">conv1_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv1_filter_size</span>  <span class="o">=</span> <span class="n">conv1_params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1_filter_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1_num_filters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1_filter_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1_num_filters</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">conv2_params</span> <span class="o">=</span> <span class="n">conv2_params</span>
            <span class="k">if</span> <span class="n">conv2_params</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv2_num_filters</span>  <span class="o">=</span> <span class="n">conv2_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv2_filter_size</span>  <span class="o">=</span> <span class="n">conv2_params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2_filter_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1_num_filters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2_num_filters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2_filter_size</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2_num_filters</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">max_pooling</span> <span class="o">=</span> <span class="n">max_pooling</span>
            <span class="k">if</span> <span class="n">max_pooling</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pooling</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span><span class="o">*</span><span class="n">seq_len</span>
                
            <span class="c1"># feed forward part at the end</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
                        
            <span class="c1">#representation to hidden</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
            
            <span class="c1">#hidden to output</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
            
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            
            <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2_params</span><span class="p">:</span>
                <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding2</span><span class="p">(</span><span class="n">representation</span><span class="p">))</span>
                        
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pooling</span><span class="p">:</span>
                <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
                        
            <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
            
            <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
            <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
            
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-the-1d-convolutional-neural-etwork">
<h3><span class="section-number">4.3.5. </span>Training the 1D convolutional neural etwork<a class="headerlink" href="#training-the-1d-convolutional-neural-etwork" title="Permalink to this headline">¶</a></h3>
<p>To train the CNN, let us reuse the same functions as in previous sections with the <code class="docutils literal notranslate"><span class="pre">FraudSequenceDataset</span></code> as <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and the <code class="docutils literal notranslate"><span class="pre">FraudConvNet</span></code> as <code class="docutils literal notranslate"><span class="pre">module</span></code>. The objective is the same as the feed-forward Network, so the criterion is binary cross-entropy as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_everything</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">training_set</span> <span class="o">=</span> <span class="n">FraudSequenceDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> 
                                    <span class="n">y_train</span><span class="p">,</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> 
                                    <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                    <span class="n">seq_len</span><span class="p">,</span>
                                    <span class="n">padding_mode</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">)</span>

<span class="n">valid_set</span> <span class="o">=</span> <span class="n">FraudSequenceDataset</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> 
                                 <span class="n">y_valid</span><span class="p">,</span>
                                 <span class="n">valid_df</span><span class="p">[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> 
                                 <span class="n">valid_df</span><span class="p">[</span><span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                 <span class="n">seq_len</span><span class="p">,</span>
                                 <span class="n">padding_mode</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">)</span>

<span class="n">training_generator</span><span class="p">,</span><span class="n">valid_generator</span> <span class="o">=</span> <span class="n">prepare_generators</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">cnn</span> <span class="o">=</span> <span class="n">FraudConvNet</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">cnn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FraudConvNet(
  (padding1): ConstantPad1d(padding=(1, 0), value=0)
  (conv1): Conv1d(15, 100, kernel_size=(2,), stride=(1,))
  (pooling): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100, out_features=100, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=100, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">cnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">cnn</span><span class="p">,</span><span class="n">training_execution_time</span><span class="p">,</span><span class="n">train_losses_dropout</span><span class="p">,</span><span class="n">valid_losses_dropout</span> <span class="o">=</span> \
    <span class="n">training_loop</span><span class="p">(</span><span class="n">cnn</span><span class="p">,</span>
                  <span class="n">training_generator</span><span class="p">,</span>
                  <span class="n">valid_generator</span><span class="p">,</span>
                  <span class="n">optimizer</span><span class="p">,</span>
                  <span class="n">criterion</span><span class="p">,</span>
                  <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0: train loss: 0.11331961992113045
valid loss: 0.04290128982539385
New best score: 0.04290128982539385

Epoch 1: train loss: 0.046289062976879895
valid loss: 0.02960259317868272
New best score: 0.02960259317868272

Epoch 2: train loss: 0.036232019433828276
valid loss: 0.026388588221743704
New best score: 0.026388588221743704

Epoch 3: train loss: 0.032827449974294105
valid loss: 0.02484128231874825
New best score: 0.02484128231874825

Epoch 4: train loss: 0.030821404086174817
valid loss: 0.02410742957730233
New best score: 0.02410742957730233

Epoch 5: train loss: 0.029202812739931062
valid loss: 0.022835184337413498
New best score: 0.022835184337413498

Epoch 6: train loss: 0.028094736857421653
valid loss: 0.02244713509854825
New best score: 0.02244713509854825

Epoch 7: train loss: 0.027001507802537853
valid loss: 0.022176400977415873
New best score: 0.022176400977415873

Epoch 8: train loss: 0.026254476560235208
valid loss: 0.02218911660570509
1  iterations since best score.

Epoch 9: train loss: 0.02560854040577751
valid loss: 0.021949108853768252
New best score: 0.021949108853768252

Epoch 10: train loss: 0.024981534799554672
valid loss: 0.021592291154964863
New best score: 0.021592291154964863

Epoch 11: train loss: 0.024556038766430716
valid loss: 0.021545066185997892
New best score: 0.021545066185997892

Epoch 12: train loss: 0.024165517638524706
valid loss: 0.02139132608751171
New best score: 0.02139132608751171

Epoch 13: train loss: 0.02384240027745459
valid loss: 0.021200239446136308
New best score: 0.021200239446136308

Epoch 14: train loss: 0.023490439055142052
valid loss: 0.021294136833313018
1  iterations since best score.

Epoch 15: train loss: 0.02323751859669618
valid loss: 0.021165060306787286
New best score: 0.021165060306787286

Epoch 16: train loss: 0.02279423619230967
valid loss: 0.021365655278354434
1  iterations since best score.

Epoch 17: train loss: 0.022614443875238456
valid loss: 0.021119751067465692
New best score: 0.021119751067465692

Epoch 18: train loss: 0.022311994288852135
valid loss: 0.021248318076062478
1  iterations since best score.

Epoch 19: train loss: 0.02212963180260797
valid loss: 0.021625581627095863
2  iterations since best score.

Epoch 20: train loss: 0.02185415759852715
valid loss: 0.021411337640742094
3  iterations since best score.
Early stopping
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluation">
<h3><span class="section-number">4.3.6. </span>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h3>
<p>To evaluate the model on the validation dataset, the command <code class="docutils literal notranslate"><span class="pre">predictions_test</span> <span class="pre">=</span> <span class="pre">model(x_test)</span></code> that we previously used on the Feed-forward Network won’t work here since the ConvNet expects the data to be in the form of sequences. The predictions need to be made properly using the validation generator. Let us implement the associated function and add it to the shared functions as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_all_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">generator</span><span class="p">:</span>
        <span class="c1"># Forward pass</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>
        <span class="c1"># append to all preds</span>
        <span class="n">all_preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">all_preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">valid_predictions</span> <span class="o">=</span> <span class="n">get_all_predictions</span><span class="p">(</span><span class="n">cnn</span><span class="p">,</span> <span class="n">valid_generator</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_df</span> <span class="o">=</span> <span class="n">valid_df</span>
<span class="n">predictions_df</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_predictions</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    
<span class="n">performance_assessment</span><span class="p">(</span><span class="n">predictions_df</span><span class="p">,</span> <span class="n">top_k_list</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AUC ROC</th>
      <th>Average precision</th>
      <th>Card Precision@100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.852</td>
      <td>0.569</td>
      <td>0.261</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Without any specific hyperparameter tuning, the performance seems to be competitive with the feed-forward neural network. A the end of this section, we’ll perform a grid search on this model for global comparison purposes.</p>
</div>
</div>
<div class="section" id="long-short-term-memory-network">
<h2><span class="section-number">4.4. </span>Long Short-Term Memory network<a class="headerlink" href="#long-short-term-memory-network" title="Permalink to this headline">¶</a></h2>
<p>As stated in the introduction, the transaction sequences can also be managed with a Long Short-Term Memory network (LSTM).</p>
<p>An LSTM is a special type of Recurrent Neural Network (RNN). The development of RNNs started early in the 80s <span id="id8">[<a class="reference internal" href="../Chapter_References/bibliography.html#id116">RHW86</a>]</span> to model data in the form of sequences (e.g. times series). The computations in an RNN are very similar to a regular feed-forward network, except that there are multiple input vectors in the form of sequence instead of a single input vector, and the RNN models the order of the vectors: it performs a succession of computations that follow the order of inputs in the sequence. In particular, it repeats a recurrent unit (a network with regular layers), from the first item to the last, that each time takes as input the output of hidden neurons (hidden state) from the previous step and the current item of the input sequence to produce a new output and a new hidden state.</p>
<p>The specificity of the LSTM is its advanced combination of the hidden state and the current sequence item to produce the new hidden state. In particular, it makes use of several gates (neurons with sigmoid activations) to cleverly select the right information to keep from the previous state and the right information to integrate from the current input. To get more into details with the specific mechanisms in the LSTM layer, we refer the reader to the following material: <a class="reference external" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<div class="section" id="lstm-for-fraud-detection">
<h3><span class="section-number">4.4.1. </span>LSTM for fraud detection<a class="headerlink" href="#lstm-for-fraud-detection" title="Permalink to this headline">¶</a></h3>
<p>LSTM have been successfully used for fraud detection in the literature <span id="id9">[<a class="reference internal" href="../Chapter_References/bibliography.html#id97">JGZ+18</a>]</span>. The key information to remember is that when the LSTM takes as input a sequence of <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> transactions, it produces a sequence of <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> hidden states of dimension <code class="docutils literal notranslate"><span class="pre">hidden_dim</span></code>. The first hidden state will be based only on an initial state of the model and the first transaction of the sequence. The second hidden state will be based on the first hidden state and the second transaction of the sequence. Therefore, one can consider that the final hidden state is an aggregated representation of the whole sequence, as long as the sequence is not too long, which can be used as input in a feed-forward layer to classify the last transaction as either fraudulent or genuine.</p>
</div>
<div class="section" id="id10">
<h3><span class="section-number">4.4.2. </span>Implementation<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>PyTorch provides a module <code class="docutils literal notranslate"><span class="pre">torch.nn.LSTM</span></code> that implements an LSTM unit. It takes as input the number of features of each vector in the sequence, the dimension of the hidden states, the number of layers, and other parameters like the percentage of dropout.</p>
<p>Let us use it to implement a module <code class="docutils literal notranslate"><span class="pre">FraudLSTM</span></code> that takes as input a sequence of transactions to predict the label of the last transaction. The first layer will be the LSTM module. Its last hidden state will be used in a fully connected network with a single hidden layer to finally predict the output neuron.</p>
<p>Note: Our <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> produces batches of dimension <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_features,</span> <span class="pre">seq_len)</span></code>. When using the option <code class="docutils literal notranslate"><span class="pre">batch_first</span> <span class="pre">=</span> <span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.nn.LSTM</span></code> expects the first dimension to be the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> which is the case. However, it expects <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> to come before <code class="docutils literal notranslate"><span class="pre">num_features</span></code>, so the second and third elements of the input must be transposed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudLSTM</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                     <span class="n">num_features</span><span class="p">,</span>
                     <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                     <span class="n">hidden_size_lstm</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                     <span class="n">num_layers_lstm</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="n">dropout_lstm</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
            
            <span class="nb">super</span><span class="p">(</span><span class="n">FraudLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="c1"># parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            
            <span class="c1"># representation learning part</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> 
                                      <span class="n">hidden_size_lstm</span><span class="p">,</span> 
                                      <span class="n">num_layers_lstm</span><span class="p">,</span> 
                                      <span class="n">batch_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
                                      <span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout_lstm</span><span class="p">)</span>
                
                        
            <span class="c1">#representation to hidden</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size_lstm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
            
            <span class="c1">#hidden to output</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
            
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            
            <span class="c1">#transposing sequence length and number of features before applying the LSTM </span>
            <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

            <span class="c1">#the second element of representation is a tuple with (final_hidden_states,final_cell_states)  </span>
            <span class="c1">#since the LSTM has 1 layer and is unidirectional, final_hidden_states has a single element</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">representation</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
            
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-the-lstm">
<h3><span class="section-number">4.4.3. </span>Training the LSTM<a class="headerlink" href="#training-the-lstm" title="Permalink to this headline">¶</a></h3>
<p>To train the LSTM, let us apply the same methodology as the CNN.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_everything</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">training_generator</span><span class="p">,</span><span class="n">valid_generator</span> <span class="o">=</span> <span class="n">prepare_generators</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">lstm</span> <span class="o">=</span> <span class="n">FraudLSTM</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lstm</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>

<span class="n">lstm</span><span class="p">,</span><span class="n">training_execution_time</span><span class="p">,</span><span class="n">train_losses_dropout</span><span class="p">,</span><span class="n">valid_losses_dropout</span> <span class="o">=</span> \
    <span class="n">training_loop</span><span class="p">(</span><span class="n">lstm</span><span class="p">,</span>
                  <span class="n">training_generator</span><span class="p">,</span>
                  <span class="n">valid_generator</span><span class="p">,</span>
                  <span class="n">optimizer</span><span class="p">,</span>
                  <span class="n">criterion</span><span class="p">,</span>
                  <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0: train loss: 0.13990207505212068
valid loss: 0.02620907245264923
New best score: 0.02620907245264923

Epoch 1: train loss: 0.031434995676729166
valid loss: 0.023325502496630034
New best score: 0.023325502496630034

Epoch 2: train loss: 0.0276708636452029
valid loss: 0.021220496156802552
New best score: 0.021220496156802552

Epoch 3: train loss: 0.025479454260691644
valid loss: 0.020511755727541943
New best score: 0.020511755727541943

Epoch 4: train loss: 0.02423322853112743
valid loss: 0.019815496125009133
New best score: 0.019815496125009133

Epoch 5: train loss: 0.02332264187745305
valid loss: 0.019972599826020294
1  iterations since best score.

Epoch 6: train loss: 0.022942402170918506
valid loss: 0.01945732499630562
New best score: 0.01945732499630562

Epoch 7: train loss: 0.02235023797337005
valid loss: 0.0196135713384217
1  iterations since best score.

Epoch 8: train loss: 0.022119645514110563
valid loss: 0.019718042683986123
2  iterations since best score.

Epoch 9: train loss: 0.021690097280103984
valid loss: 0.01908009442885004
New best score: 0.01908009442885004

Epoch 10: train loss: 0.02134914275318434
valid loss: 0.01881876519583471
New best score: 0.01881876519583471

Epoch 11: train loss: 0.02092900848780522
valid loss: 0.019134794213030427
1  iterations since best score.

Epoch 12: train loss: 0.02074213598841039
valid loss: 0.019468843063232717
2  iterations since best score.

Epoch 13: train loss: 0.020374752355523114
valid loss: 0.01866684172651094
New best score: 0.01866684172651094

Epoch 14: train loss: 0.02008993904532359
valid loss: 0.01853460792039872
New best score: 0.01853460792039872

Epoch 15: train loss: 0.019746437735577785
valid loss: 0.01839204282675427
New best score: 0.01839204282675427

Epoch 16: train loss: 0.019563284586295023
valid loss: 0.01872969562482964
1  iterations since best score.

Epoch 17: train loss: 0.019375868797962006
valid loss: 0.01919776629834675
2  iterations since best score.

Epoch 18: train loss: 0.019136815694366434
valid loss: 0.017995675191311216
New best score: 0.017995675191311216

Epoch 19: train loss: 0.018896986096734573
valid loss: 0.01803255443196601
1  iterations since best score.

Epoch 20: train loss: 0.01882533677201451
valid loss: 0.018077422815306325
2  iterations since best score.

Epoch 21: train loss: 0.01855331002204834
valid loss: 0.018072079796384755
3  iterations since best score.
Early stopping
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id11">
<h3><span class="section-number">4.4.4. </span>Evaluation<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>Evaluation is also the same as with the CNN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">valid_predictions</span> <span class="o">=</span> <span class="n">get_all_predictions</span><span class="p">(</span><span class="n">lstm</span><span class="p">,</span> <span class="n">valid_generator</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_df</span> <span class="o">=</span> <span class="n">valid_df</span>
<span class="n">predictions_df</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_predictions</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    
<span class="n">performance_assessment</span><span class="p">(</span><span class="n">predictions_df</span><span class="p">,</span> <span class="n">top_k_list</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AUC ROC</th>
      <th>Average precision</th>
      <th>Card Precision@100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.857</td>
      <td>0.658</td>
      <td>0.279</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>This first result with the LSTM is very encouraging and highly competitive with the other architectures tested in the chapter.</p>
<p>It is worth noting that in this example, only the last hidden state is used in the final classification network. When dealing with long sequences and complex patterns, this state alone can become limited to integrate all the useful information for fraud detection. Moreover, it makes it difficult to identify the contribution of the different parts of the sequence to a specific prediction. To deal with these issues, one can use all the hidden states from the LSTM and resort to Attention to select and combine them.</p>
</div>
</div>
<div class="section" id="towards-more-advanced-modeling-with-attention">
<h2><span class="section-number">4.5. </span>Towards more advanced modeling with Attention<a class="headerlink" href="#towards-more-advanced-modeling-with-attention" title="Permalink to this headline">¶</a></h2>
<p>The Attention mechanism is one of the major recent breakthroughs in neural network architectures <span id="id12">[<a class="reference internal" href="../Chapter_References/bibliography.html#id114">BCB14</a>]</span>. It has led to significant advances in Natural Language Processing (NLP), for instance, in the <em>Transformer architecture</em> <span id="id13">[<a class="reference internal" href="../Chapter_References/bibliography.html#id117">VSP+17</a>]</span> and its multiple variants, e.g. into BERT <span id="id14">[<a class="reference internal" href="../Chapter_References/bibliography.html#id118">DCLT18</a>]</span> or GPT <span id="id15">[<a class="reference internal" href="../Chapter_References/bibliography.html#id119">RWC+19</a>]</span>.</p>
<p>The Attention mechanism is an implementation of the concept of Attention, namely selectively focusing on a subset of relevant items (e.g. states) in deep neural networks. It was initially proposed as an additional layer to improve the classical encoder-decoder LSTM architecture for neural machine translation. It allows aligning the usage of the encoder hidden states and the element currently being generated by the decoder, and solving the long-range dependency problem of LSTMs.</p>
<p>The difference with the regular usage of an LSTM is that instead of only using the last hidden state, the Attention mechanism takes as input all the hidden states and combines them in a relevant manner with respect to a certain context. More precisely, in its most popular form, Attention performs the following operations:</p>
<ul class="simple">
<li><p>Given a context vector <span class="math notranslate nohighlight">\(c\)</span> and the sequence of hidden states <span class="math notranslate nohighlight">\(h_i\)</span>, it computes an attention score <span class="math notranslate nohighlight">\(a_i\)</span> for each hidden state, generally using a similarity measure like a dot product between <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(h_i\)</span>.</p></li>
<li><p>It normalizes all the attention scores with a softmax.</p></li>
<li><p>It computes a global output state with a linear combination <span class="math notranslate nohighlight">\(\sum a_i*h_i\)</span>.</p></li>
</ul>
<p>For applications like machine translation with an encoder-decoder architecture, the context vector will generally be the current hidden state of the decoder, and the Attention will be applied to all hidden states of the encoder. In such application, the encoder LSTM takes as input a sentence (sequence of words) in a language (e.g. French), and the decoder LSTM takes as input the beginning of the translated sentence in another language (e.g. English). Therefore, it makes sense to consider the current state of the translation as context to select/focus the right elements of the input sequence that will be taken into account to predict the next word of the translation.</p>
<div class="section" id="attention-for-fraud-detection">
<h3><span class="section-number">4.5.1. </span>Attention for fraud detection<a class="headerlink" href="#attention-for-fraud-detection" title="Permalink to this headline">¶</a></h3>
<p>For fraud detection, only an encoder LSTM is used in our implementation above. The choice of a relevant context vector will therefore be based on our intuition of what kind of context makes sense to select the right hidden states of the sequence. A reasonable choice is to consider a representation of the transaction that we aim at classifying (the last transaction) as context to select the right elements from the previous transactions. Two choices are possible: directly use the last hidden state as a context vector, or a projection of the last transaction (e.g. after applying a <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> layer). In the following, we resort to the second option, which is represented in the global architecture below:</p>
<p><img alt="Illustration of a sequential model with Attention" src="../_images/attention.png" /></p>
<p>Additionally to allow dynamic selection of the relevant hidden states for the sample at hand, the attention scores can provide interpretability by showing the parts of the sequence used for the current prediction.</p>
</div>
<div class="section" id="id16">
<h3><span class="section-number">4.5.2. </span>Implementation<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>There is no native implementation of a simple Attention layer in the current Pytorch version (1.9). However, a more general <code class="docutils literal notranslate"><span class="pre">torch.nn.MultiheadAttention</span></code>, like the one used in the Transformer architecture, is available. Although it would allow implementing regular attention, we will instead make use of an unofficial module that implements a simpler attention mechanism for educational purposes. This Attention module is available in the following widely validated git repository: <a class="reference external" href="https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/attention.py">https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/attention.py</a></p>
<p>Let us copy its content in the following cell:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># source : https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/attention.py</span>

<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>


<span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies an attention mechanism on the output features from the decoder.</span>
<span class="sd">    .. math::</span>
<span class="sd">            \begin{array}{ll}</span>
<span class="sd">            x = context*output \\</span>
<span class="sd">            attn = exp(x_i) / sum_j exp(x_j) \\</span>
<span class="sd">            output = \tanh(w * (attn * context) + b * output)</span>
<span class="sd">            \end{array}</span>
<span class="sd">    Args:</span>
<span class="sd">        dim(int): The number of expected features in the output</span>
<span class="sd">    Inputs: output, context</span>
<span class="sd">        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.</span>
<span class="sd">        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.</span>
<span class="sd">    Outputs: output, attn</span>
<span class="sd">        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.</span>
<span class="sd">        - **attn** (batch, output_len, input_len): tensor containing attention weights.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.</span>
<span class="sd">        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.</span>
<span class="sd">    Examples::</span>
<span class="sd">         &gt;&gt;&gt; attention = seq2seq.models.Attention(256)</span>
<span class="sd">         &gt;&gt;&gt; context = Variable(torch.randn(5, 3, 256))</span>
<span class="sd">         &gt;&gt;&gt; output = Variable(torch.randn(5, 5, 256))</span>
<span class="sd">         &gt;&gt;&gt; output, attn = attention(output, context)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">set_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets indices to be masked</span>
<span class="sd">        Args:</span>
<span class="sd">            mask (torch.Tensor): tensor containing indices to be masked</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># (batch, out_len, dim) * (batch, in_len, dim) -&gt; (batch, out_len, in_len)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>

        <span class="c1"># (batch, out_len, in_len) * (batch, in_len, dim) -&gt; (batch, out_len, dim)</span>
        <span class="n">mix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

        <span class="c1"># concat -&gt; (batch, out_len, 2*dim)</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">mix</span><span class="p">,</span> <span class="n">output</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># output -&gt; (batch, out_len, dim)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_out</span><span class="p">(</span><span class="n">combined</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">)))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="how-does-it-work">
<h3><span class="section-number">4.5.3. </span>How does it work?<a class="headerlink" href="#how-does-it-work" title="Permalink to this headline">¶</a></h3>
<p>The custom <code class="docutils literal notranslate"><span class="pre">Attention</span></code> module above has a single initialization parameter which is the dimension of the input hidden states and the context vector. During the forward pass, it takes as input the sequence of hidden states and the context vector and outputs the combined state and the attention scores.</p>
<p>To familiarize with the module, let us manually compute the hidden states of our previous LSTM on a random training batch, and test the Attention mechanism.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">training_generator</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out_seq</span><span class="p">,</span> <span class="p">(</span><span class="n">last_hidden</span><span class="p">,</span><span class="n">last_cell</span><span class="p">)</span> <span class="o">=</span> <span class="n">lstm</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x_batch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The outputs of the LSTM are the sequence of all hidden states, and the last hidden and cell states.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">last_hidden</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 64, 100])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out_seq</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([64, 5, 100])
</pre></div>
</div>
</div>
</div>
<p>Let us store the sequences of hidden states in a variable <code class="docutils literal notranslate"><span class="pre">test_hidden_states_seq</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_hidden_states_seq</span> <span class="o">=</span> <span class="n">out_seq</span>
</pre></div>
</div>
</div>
</div>
<p>To create our context vector, let us apply a fully connected layer on the last element of the input sequence (which is the transaction that we aim at classifying) and store the result in a variable <code class="docutils literal notranslate"><span class="pre">test_context_vector</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_context_projector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">x_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_seq</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_context_vector</span> <span class="o">=</span> <span class="n">test_context_projector</span><span class="p">(</span><span class="n">x_batch</span><span class="p">[:,:,</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The hidden states and context vectors of the whole batch have the following dimensions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_hidden_states_seq</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([64, 5, 100])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_context_vector</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([64, 1, 100])
</pre></div>
</div>
</div>
</div>
<p>Now that the inputs for the Attention mechanism are ready, let us try the module:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_everything</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">test_attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_state</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">test_attention</span><span class="p">(</span><span class="n">test_context_vector</span><span class="p">,</span><span class="n">test_hidden_states_seq</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_state</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([64, 1, 100])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attn</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.2517, 0.2583, 0.2432, 0.1509, 0.0959], device=&#39;cuda:0&#39;,
       grad_fn=&lt;SelectBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p>We obtain two outputs. <code class="docutils literal notranslate"><span class="pre">attn</span></code> contains the attention scores of the hidden states and <code class="docutils literal notranslate"><span class="pre">output_state</span></code> is the output combined state, i.e. the linear combination of the hidden states based on the attention scores.</p>
<p>Here the components of <code class="docutils literal notranslate"><span class="pre">attn</span></code> are rather balanced because, since the module <code class="docutils literal notranslate"><span class="pre">test_context_projector</span></code> was only randomly initialized, the context vectors <code class="docutils literal notranslate"><span class="pre">test_context_vector</span></code> are “random” and not specifically more similar to a state than another.</p>
<p>Let us see what happens if the last hidden state <code class="docutils literal notranslate"><span class="pre">test_hidden_states_seq[:,4:,:]</span></code> is used as context vector instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">test_attention</span><span class="p">(</span><span class="n">test_hidden_states_seq</span><span class="p">[:,</span><span class="mi">4</span><span class="p">:,:],</span><span class="n">test_hidden_states_seq</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attn</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([7.5513e-09, 4.9199e-08, 1.3565e-06, 1.2371e-03, 9.9876e-01],
       device=&#39;cuda:0&#39;, grad_fn=&lt;SelectBackward&gt;)
</pre></div>
</div>
</div>
</div>
<p>This time, it is clear that the attention score is much larger for the last transaction since it is equal to the context vector. Interestingly, one can observe that the scores decrease from the most recent previous transaction to the oldest.</p>
<p>Nevertheless, using the last hidden state as a context vector will not necessarily guarantee better behavior on fraud classification. Instead, let us hold on to our strategy with a feed-forward layer that will compute a context vector from the last transaction and train this layer, the LSTM and the final classifier (which takes as input the combined state to classify the transaction) altogether. To do so, we implement a custom module <code class="docutils literal notranslate"><span class="pre">FraudLSTMWithAttention</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudLSTMWithAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                     <span class="n">num_features</span><span class="p">,</span>
                     <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                     <span class="n">hidden_size_lstm</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                     <span class="n">num_layers_lstm</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="n">dropout_lstm</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
                     <span class="n">attention_out_dim</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
            
            <span class="nb">super</span><span class="p">(</span><span class="n">FraudLSTMWithAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="c1"># parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            
            <span class="c1"># sequence representation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> 
                                      <span class="n">hidden_size_lstm</span><span class="p">,</span> 
                                      <span class="n">num_layers_lstm</span><span class="p">,</span> 
                                      <span class="n">batch_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
                                      <span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout_lstm</span><span class="p">)</span>
            
            <span class="c1"># layer that will project the last transaction of the sequence into a context vector</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">hidden_size_lstm</span><span class="p">)</span>
            
            <span class="c1"># attention layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">attention_out_dim</span><span class="p">)</span>
                        
            <span class="c1">#representation to hidden</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size_lstm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
            
            <span class="c1">#hidden to output</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
            
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            
            <span class="c1">#computing the sequence of hidden states from the sequence of transactions</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
            
            <span class="c1">#computing the context vector from the last transaction</span>
            <span class="n">context_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">[:,:,</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
            
            <span class="n">combined_state</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>

                        
            <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">combined_state</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:])</span>
            <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
            
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-the-lstm-with-attention">
<h3><span class="section-number">4.5.4. </span>Training the LSTM with Attention<a class="headerlink" href="#training-the-lstm-with-attention" title="Permalink to this headline">¶</a></h3>
<p>The LSTM with Attention takes the same input as the regular LSTM so it can be trained and evaluated in the exact same manner.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_everything</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">lstm_attn</span> <span class="o">=</span> <span class="n">FraudLSTMWithAttention</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">lstm_attn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FraudLSTMWithAttention(
  (lstm): LSTM(15, 100, batch_first=True)
  (ff): Linear(in_features=15, out_features=100, bias=True)
  (attention): Attention(
    (linear_out): Linear(in_features=200, out_features=100, bias=True)
  )
  (fc1): Linear(in_features=100, out_features=100, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=100, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_generator</span><span class="p">,</span><span class="n">valid_generator</span> <span class="o">=</span> <span class="n">prepare_generators</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span><span class="n">valid_set</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lstm_attn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00008</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="n">lstm_attn</span><span class="p">,</span><span class="n">training_execution_time</span><span class="p">,</span><span class="n">train_losses_dropout</span><span class="p">,</span><span class="n">valid_losses_dropout</span> <span class="o">=</span> \
    <span class="n">training_loop</span><span class="p">(</span><span class="n">lstm_attn</span><span class="p">,</span>
                  <span class="n">training_generator</span><span class="p">,</span>
                  <span class="n">valid_generator</span><span class="p">,</span>
                  <span class="n">optimizer</span><span class="p">,</span>
                  <span class="n">criterion</span><span class="p">,</span>
                  <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0: train loss: 0.10238753851141645
valid loss: 0.021834761867951094
New best score: 0.021834761867951094

Epoch 1: train loss: 0.026330269543929505
valid loss: 0.0203155988219189
New best score: 0.0203155988219189

Epoch 2: train loss: 0.024288250049589517
valid loss: 0.019749290624867535
New best score: 0.019749290624867535

Epoch 3: train loss: 0.02330792175176737
valid loss: 0.019204635951856324
New best score: 0.019204635951856324

Epoch 4: train loss: 0.02269919227573212
valid loss: 0.019130825754789423
New best score: 0.019130825754789423

Epoch 5: train loss: 0.022160232767046928
valid loss: 0.018929402052572434
New best score: 0.018929402052572434

Epoch 6: train loss: 0.02177732186309591
valid loss: 0.01847629409672723
New best score: 0.01847629409672723

Epoch 7: train loss: 0.02135627254457293
valid loss: 0.018755287848173798
1  iterations since best score.

Epoch 8: train loss: 0.02098137940145249
valid loss: 0.01887945729890543
2  iterations since best score.

Epoch 9: train loss: 0.020458271793019914
valid loss: 0.01891031490531979
3  iterations since best score.
Early stopping
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="validation">
<h3><span class="section-number">4.5.5. </span>Validation<a class="headerlink" href="#validation" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">valid_predictions</span> <span class="o">=</span> <span class="n">get_all_predictions</span><span class="p">(</span><span class="n">lstm_attn</span><span class="p">,</span> <span class="n">valid_generator</span><span class="p">)</span>

<span class="n">predictions_df</span> <span class="o">=</span> <span class="n">valid_df</span>
<span class="n">predictions_df</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_predictions</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    
<span class="n">performance_assessment</span><span class="p">(</span><span class="n">predictions_df</span><span class="p">,</span> <span class="n">top_k_list</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AUC ROC</th>
      <th>Average precision</th>
      <th>Card Precision@100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.855</td>
      <td>0.635</td>
      <td>0.271</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The results are competitive with the LSTM. Additionnaly, the advantage with this architecture is the interpretability of the attention scores for a given prediction.</p>
<p>In other settings, for instance with longer sequences, this model might also be able to reach better performance thant the regular LSTM.</p>
</div>
</div>
<div class="section" id="seq-2-seq-autoencoders">
<h2><span class="section-number">4.6. </span>Seq-2-Seq Autoencoders<a class="headerlink" href="#seq-2-seq-autoencoders" title="Permalink to this headline">¶</a></h2>
<p>The previous section of this chapter covered the use of regular autoencoders on single transactions for fraud detection. It is worth noting that the same principles could be applied here to create a semi-supervised method for sequential inputs. Instead of a feed-forward architecture, a sequence-to-sequence autoencoder with an encoder-decoder architecture could be used. This is not covered in this version of the section, but it will be proposed in the future.</p>
</div>
<div class="section" id="prequential-grid-search">
<h2><span class="section-number">4.7. </span>Prequential grid search<a class="headerlink" href="#prequential-grid-search" title="Permalink to this headline">¶</a></h2>
<p>Now that we have explored different sequential models architectures, let us finally evaluate them properly with a prequential grid search. Compared to the grid search performed on the feed-forward neural network, the sequential models add a little complexity to the process. Indeed, they have been designed to take as input a sequence of transactions. For this purpose, the specific <code class="docutils literal notranslate"><span class="pre">FraudSequenceDataset</span></code> was implemented, and it requires two additional features to build the sequences: the landmark feature (<code class="docutils literal notranslate"><span class="pre">CUSTOMER_ID</span></code>) and the chronological feature (<code class="docutils literal notranslate"><span class="pre">TX_DATETIME</span></code>). Our previous model selection function (<code class="docutils literal notranslate"><span class="pre">model_selection_wrapper</span></code>) does not directly allow passing these extra parameters to the <code class="docutils literal notranslate"><span class="pre">torch</span></code> Dataset. The trick here will be to simply pass them as regular features but only use them to build the sequences. For that, the <code class="docutils literal notranslate"><span class="pre">FraudSequenceDataset</span></code> needs to be updated into a new version (that will be referred to as <code class="docutils literal notranslate"><span class="pre">FraudSequenceDatasetForPipe</span></code>) that only takes as input <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and assumes that the last column of <code class="docutils literal notranslate"><span class="pre">x</span></code> is <code class="docutils literal notranslate"><span class="pre">TX_DATETIME</span></code>, the previous column is <code class="docutils literal notranslate"><span class="pre">CUSTOMER_ID</span></code>, and the rest are the transactions regular features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudSequenceDatasetForPipe</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="s1">&#39;Initialization&#39;</span>
        <span class="n">seq_len</span><span class="o">=</span><span class="mi">5</span>
        
        <span class="c1"># lets us assume that x[:,-1] are the dates, and x[:,-2] are customer ids, padding_mode is &quot;mean&quot;</span>
        <span class="n">customer_ids</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">dates</span>  <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> 
        
        <span class="c1"># storing the features x in self.feature and adding the &quot;padding&quot; transaction at the end</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
                
        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
            
        <span class="bp">self</span><span class="o">.</span><span class="n">customer_ids</span> <span class="o">=</span> <span class="n">customer_ids</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dates</span> <span class="o">=</span> <span class="n">dates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
        
        <span class="c1">#===== computing sequences ids =====       </span>
        
        
        <span class="n">df_ids_dates_cpy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">:</span><span class="n">customer_ids</span><span class="p">,</span>
        <span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">:</span><span class="n">dates</span><span class="p">})</span>
        
        <span class="n">df_ids_dates_cpy</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_ids_dates_cpy</span><span class="p">))</span>
        <span class="n">df_groupby_customer_id</span> <span class="o">=</span> <span class="n">df_ids_dates_cpy</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;CUSTOMER_ID&quot;</span><span class="p">)</span>
        <span class="n">sequence_indices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;tx_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">):</span> <span class="n">df_groupby_customer_id</span><span class="p">[</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sequences_ids</span> <span class="o">=</span> <span class="n">sequence_indices</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        
        <span class="n">df_ids_dates_cpy</span> <span class="o">=</span> <span class="n">df_ids_dates_cpy</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;tmp_index&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        


    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s1">&#39;Denotes the total number of samples&#39;</span>
        <span class="c1"># not len(self.features) because of the added padding transaction</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">customer_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="s1">&#39;Generates one sample of data&#39;</span>
        <span class="c1"># Select sample index</span>
        
        <span class="n">tx_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequences_ids</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1">#transposing because the CNN considers the channel dimension before the sequence dimension</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">tx_ids</span><span class="p">,:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">tx_ids</span><span class="p">,:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="grid-search-on-the-1-d-convolutional-neural-network">
<h3><span class="section-number">4.7.1. </span>Grid search on the 1-D Convolutional Neural Network<a class="headerlink" href="#grid-search-on-the-1-d-convolutional-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Let us perform a grid search on the 1-D CNN with the following hyperparameters:</p>
<ul class="simple">
<li><p>Batch size : [64,128,256]</p></li>
<li><p>Initial learning rate: [0.0001, 0.0002, 0.001]</p></li>
<li><p>Number of epochs : [10, 20, 40]</p></li>
<li><p>Dropout rate : [0, 0.2]</p></li>
<li><p>Number of convolutional layers : [1,2]</p></li>
<li><p>Number of convolutional filters : [100, 200]</p></li>
</ul>
<p>For that, the <code class="docutils literal notranslate"><span class="pre">FraudCNN</span></code> module needs to be adapted to output two probabilities like <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> classifiers, and then wrapped with <code class="docutils literal notranslate"><span class="pre">skorch</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudCNN</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num_filters</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">filter_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_conv</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_pooling</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FraudCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        
        <span class="c1"># representation learning part</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span>  <span class="o">=</span> <span class="n">num_filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span>  <span class="o">=</span> <span class="n">filter_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">((</span><span class="n">filter_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">num_conv</span><span class="o">=</span><span class="n">num_conv</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_conv</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">((</span><span class="n">filter_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pooling</span> <span class="o">=</span> <span class="n">max_pooling</span>
        <span class="k">if</span> <span class="n">max_pooling</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span><span class="o">*</span><span class="n">seq_len</span>
            
        <span class="c1"># feed forward part at the end</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
                    
        <span class="c1">#representation to hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">representation_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        
        <span class="c1">#hidden to output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>                         
        <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_conv</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
            <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding2</span><span class="p">(</span><span class="n">representation</span><span class="p">))</span>                         
            <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
                    
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pooling</span><span class="p">:</span>
            <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
                    
        <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
        
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">representation</span><span class="p">)</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>                                     
        <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<p>The two extra features (<code class="docutils literal notranslate"><span class="pre">CUSTOMER_ID</span></code> and <code class="docutils literal notranslate"><span class="pre">TX_DATETIME</span></code>) also need to be added to the list <code class="docutils literal notranslate"><span class="pre">input_features</span></code>.</p>
<p>Note: the <code class="docutils literal notranslate"><span class="pre">model_selection_wrapper</span></code> function implements an sklearn pipeline that combines the classifier with a scaler. Therefore, the two extra variables will be standardized like the rest of the features. To avoid unexpected behavior, let us convert the datetimes into timestamps. Once done, the normalization of both <code class="docutils literal notranslate"><span class="pre">CUSTOMER_ID</span></code> and <code class="docutils literal notranslate"><span class="pre">TX_DATETIME_TIMESTAMP</span></code> should not change the set of unique ids of customers nor the chronological order of the transactions, and therefore lead to the same sequences and results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transactions_df</span><span class="p">[</span><span class="s1">&#39;TX_DATETIME_TIMESTAMP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transactions_df</span><span class="p">[</span><span class="s1">&#39;TX_DATETIME&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">timestamp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">input_features_new</span> <span class="o">=</span> <span class="n">input_features</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">,</span><span class="s1">&#39;TX_DATETIME_TIMESTAMP&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now that all the classes are ready, let us run the grid search with the CNN using the skorch wrapper and the same scoring and validation settings as in previous sections.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install skorch
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: skorch in /opt/conda/lib/python3.8/site-packages (0.10.0)
Requirement already satisfied: scipy&gt;=1.1.0 in /opt/conda/lib/python3.8/site-packages (from skorch) (1.6.3)
Requirement already satisfied: tabulate&gt;=0.7.7 in /opt/conda/lib/python3.8/site-packages (from skorch) (0.8.9)
Requirement already satisfied: tqdm&gt;=4.14.0 in /opt/conda/lib/python3.8/site-packages (from skorch) (4.51.0)
Requirement already satisfied: numpy&gt;=1.13.3 in /opt/conda/lib/python3.8/site-packages (from skorch) (1.19.2)
Requirement already satisfied: scikit-learn&gt;=0.19.1 in /opt/conda/lib/python3.8/site-packages (from skorch) (0.24.2)
Requirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn&gt;=0.19.1-&gt;skorch) (1.0.1)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn&gt;=0.19.1-&gt;skorch) (2.1.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">skorch</span> <span class="kn">import</span> <span class="n">NeuralNetClassifier</span>

<span class="c1"># Only keep columns that are needed as argument to custome scoring function</span>
<span class="c1"># to reduce serialisation time of transaction dataset</span>
<span class="n">transactions_df_scorer</span> <span class="o">=</span> <span class="n">transactions_df</span><span class="p">[[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">,</span> <span class="s1">&#39;TX_FRAUD&#39;</span><span class="p">,</span><span class="s1">&#39;TX_TIME_DAYS&#39;</span><span class="p">]]</span>

<span class="n">card_precision_top_100</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">card_precision_top_k_custom</span><span class="p">,</span> 
                                                     <span class="n">needs_proba</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                                     <span class="n">top_k</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                                                     <span class="n">transactions_df</span><span class="o">=</span><span class="n">transactions_df_scorer</span><span class="p">)</span>

<span class="n">performance_metrics_list_grid</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">,</span> <span class="s1">&#39;card_precision@100&#39;</span><span class="p">]</span>
<span class="n">performance_metrics_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">]</span>

<span class="n">scoring</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">:</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span>
           <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">,</span>
           <span class="s1">&#39;card_precision@100&#39;</span><span class="p">:</span> <span class="n">card_precision_top_100</span><span class="p">,</span>
           <span class="p">}</span>

<span class="n">n_folds</span><span class="o">=</span><span class="mi">4</span>
<span class="n">start_date_training_for_valid</span> <span class="o">=</span> <span class="n">start_date_training</span><span class="o">+</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=-</span><span class="p">(</span><span class="n">delta_delay</span><span class="o">+</span><span class="n">delta_valid</span><span class="p">))</span>
<span class="n">start_date_training_for_test</span> <span class="o">=</span> <span class="n">start_date_training</span><span class="o">+</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="p">(</span><span class="n">n_folds</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">delta_test</span><span class="p">)</span>
<span class="n">delta_assessment</span> <span class="o">=</span> <span class="n">delta_valid</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_everything</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">NeuralNetClassifier</span><span class="p">(</span>
    <span class="n">FraudCNN</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">FraudSequenceDatasetForPipe</span><span class="p">,</span>
    <span class="n">iterator_train__shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">train_split</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;clf__lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span><span class="mf">0.0002</span><span class="p">,</span><span class="mf">0.001</span><span class="p">],</span>
    <span class="s1">&#39;clf__batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span>
    <span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__hidden_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">500</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__num_conv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__p&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__num_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">))],</span>
    <span class="s1">&#39;clf__module__num_filters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">start_time</span><span class="o">=</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="n">performances_df</span><span class="o">=</span><span class="n">model_selection_wrapper</span><span class="p">(</span><span class="n">transactions_df</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> 
                                        <span class="n">input_features_new</span><span class="p">,</span> <span class="n">output_feature</span><span class="p">,</span>
                                        <span class="n">parameters</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> 
                                        <span class="n">start_date_training_for_valid</span><span class="p">,</span>
                                        <span class="n">start_date_training_for_test</span><span class="p">,</span>
                                        <span class="n">n_folds</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span>
                                        <span class="n">delta_train</span><span class="o">=</span><span class="n">delta_train</span><span class="p">,</span> 
                                        <span class="n">delta_delay</span><span class="o">=</span><span class="n">delta_delay</span><span class="p">,</span> 
                                        <span class="n">delta_assessment</span><span class="o">=</span><span class="n">delta_assessment</span><span class="p">,</span>
                                        <span class="n">performance_metrics_list_grid</span><span class="o">=</span><span class="n">performance_metrics_list_grid</span><span class="p">,</span>
                                        <span class="n">performance_metrics_list</span><span class="o">=</span><span class="n">performance_metrics_list</span><span class="p">,</span>
                                        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">execution_time_cnn</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span>

<span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__num_conv&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>    
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__num_filters&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>                                       
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>

<span class="n">performances_df_cnn</span><span class="o">=</span><span class="n">performances_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">execution_time_cnn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>26904.69369339943
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summary_performances_cnn</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_cnn</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">summary_performances_cnn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AUC ROC</th>
      <th>Average precision</th>
      <th>Card Precision@100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Best estimated parameters</th>
      <td>20/2/64/100/0.2</td>
      <td>40/2/64/100/0.2</td>
      <td>10/2/128/100/0</td>
    </tr>
    <tr>
      <th>Validation performance</th>
      <td>0.879+/-0.01</td>
      <td>0.613+/-0.02</td>
      <td>0.279+/-0.01</td>
    </tr>
    <tr>
      <th>Test performance</th>
      <td>0.872+/-0.01</td>
      <td>0.599+/-0.01</td>
      <td>0.288+/-0.01</td>
    </tr>
    <tr>
      <th>Optimal parameter(s)</th>
      <td>10/2/64/100/0.2</td>
      <td>20/2/64/100/0.2</td>
      <td>20/2/128/100/0.2</td>
    </tr>
    <tr>
      <th>Optimal test performance</th>
      <td>0.876+/-0.01</td>
      <td>0.617+/-0.01</td>
      <td>0.296+/-0.01</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The results of the CNN on the simulated data are slightly less convincing than the feed-forward network. There can be several reasons for that. In particular, with regards to the patterns annotated as frauds in the simulated data, the aggregates in <code class="docutils literal notranslate"><span class="pre">input_features</span></code> may already provide enough context to regular models, which limits the interest of contextualization with the sequence. Let us have a look at the impact of some hyperparameters to better understand our model. Let us fix the number of convolutional layers to 2, the dropout level to 0.2, and visualize the impact of batch size, number of epochs, and number of filters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df_cnn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df_cnn</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>    
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__num_filters&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>

<span class="n">performances_df_cnn_subset</span> <span class="o">=</span> <span class="n">performances_df_cnn</span><span class="p">[</span><span class="n">performances_df_cnn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__lr&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mf">0.001</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__num_filters&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">100</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">20</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__num_conv&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">summary_performances_cnn_subset</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_cnn_subset</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">indexes_summary</span> <span class="o">=</span> <span class="n">summary_performances_cnn_subset</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
<span class="n">indexes_summary</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Best estimated parameters&#39;</span>
<span class="n">summary_performances_cnn_subset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes_summary</span><span class="p">)),</span><span class="n">indexes_summary</span><span class="p">)))</span>
<span class="n">get_performances_plots</span><span class="p">(</span><span class="n">performances_df_cnn_subset</span><span class="p">,</span> 
                       <span class="n">performance_metrics_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">],</span> 
                       <span class="n">expe_type_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">expe_type_color_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#008000&#39;</span><span class="p">,</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">],</span>
                       <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;batch size&quot;</span><span class="p">,</span>
                       <span class="n">summary_performances</span><span class="o">=</span><span class="n">summary_performances_cnn_subset</span><span class="p">)</span>

<span class="n">performances_df_cnn_subset</span> <span class="o">=</span> <span class="n">performances_df_cnn</span><span class="p">[</span><span class="n">performances_df_cnn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__lr&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mf">0.001</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__num_filters&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">100</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">64</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__num_conv&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">summary_performances_cnn_subset</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_cnn_subset</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">indexes_summary</span> <span class="o">=</span> <span class="n">summary_performances_cnn_subset</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
<span class="n">indexes_summary</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Best estimated parameters&#39;</span>
<span class="n">summary_performances_cnn_subset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes_summary</span><span class="p">)),</span><span class="n">indexes_summary</span><span class="p">)))</span>
<span class="n">get_performances_plots</span><span class="p">(</span><span class="n">performances_df_cnn_subset</span><span class="p">,</span> 
                       <span class="n">performance_metrics_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">],</span> 
                       <span class="n">expe_type_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">expe_type_color_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#008000&#39;</span><span class="p">,</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">],</span>
                       <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span>
                       <span class="n">summary_performances</span><span class="o">=</span><span class="n">summary_performances_cnn_subset</span><span class="p">)</span>

<span class="n">performances_df_cnn_subset</span> <span class="o">=</span> <span class="n">performances_df_cnn</span><span class="p">[</span><span class="n">performances_df_cnn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__lr&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mf">0.001</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">20</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">64</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__num_conv&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">summary_performances_cnn_subset</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_cnn_subset</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">indexes_summary</span> <span class="o">=</span> <span class="n">summary_performances_cnn_subset</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
<span class="n">indexes_summary</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Best estimated parameters&#39;</span>
<span class="n">summary_performances_cnn_subset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes_summary</span><span class="p">)),</span><span class="n">indexes_summary</span><span class="p">)))</span>
<span class="n">get_performances_plots</span><span class="p">(</span><span class="n">performances_df_cnn_subset</span><span class="p">,</span> 
                       <span class="n">performance_metrics_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">],</span> 
                       <span class="n">expe_type_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">expe_type_color_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#008000&#39;</span><span class="p">,</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">],</span>
                       <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;num filters&quot;</span><span class="p">,</span>
                       <span class="n">summary_performances</span><span class="o">=</span><span class="n">summary_performances_cnn_subset</span><span class="p">)</span>


<span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df_cnn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df_cnn</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__num_conv&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>    
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__num_filters&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>                                       
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/SequentialModeling_123_0.png" src="../_images/SequentialModeling_123_0.png" />
<img alt="../_images/SequentialModeling_123_1.png" src="../_images/SequentialModeling_123_1.png" />
<img alt="../_images/SequentialModeling_123_2.png" src="../_images/SequentialModeling_123_2.png" />
</div>
</div>
<p>Similar to the feed-forward network, the number of epochs and batch size, which are optimization parameters, have a sweet spot, which is probably connected to other parameters (model size, dropout, etc.). For the chosen optimization parameters, 100 filters seem to lead to better results than 200.</p>
</div>
</div>
<div class="section" id="grid-search-on-the-long-short-term-memory">
<h2><span class="section-number">4.8. </span>Grid search on the Long Short Term Memory<a class="headerlink" href="#grid-search-on-the-long-short-term-memory" title="Permalink to this headline">¶</a></h2>
<p>For the LSTM, we will search the following hyperparameters:</p>
<ul class="simple">
<li><p>Batch size : [64,128,256]</p></li>
<li><p>Initial learning rate: [0.0001, 0.0002, 0.001]</p></li>
<li><p>Number of epochs : [5, 10, 20]</p></li>
<li><p>Dropout rate : [0, 0.2, 0.4]</p></li>
<li><p>Dimension of the LSTM hidden states : [100,200]</p></li>
</ul>
<p>The LSTM takes sequences of transactions as input, so the process is the same as for the CNN, and the module also needs to be adapted to output two neurons.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudLSTM</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">hidden_size_lstm</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num_layers_lstm</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FraudLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># representation learning part</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">hidden_size_lstm</span><span class="p">,</span> <span class="n">num_layers_lstm</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="n">p</span><span class="p">)</span>
            
                    
        <span class="c1">#representation to hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size_lstm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        
        <span class="c1">#hidden to output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="n">representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

                    
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">representation</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_everything</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">NeuralNetClassifier</span><span class="p">(</span>
    <span class="n">FraudLSTM</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">FraudSequenceDatasetForPipe</span><span class="p">,</span>
    <span class="n">iterator_train__shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">train_split</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;clf__lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span><span class="mf">0.0002</span><span class="p">,</span><span class="mf">0.001</span><span class="p">],</span>
    <span class="s1">&#39;clf__batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span>
    <span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__hidden_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">500</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__p&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__num_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">))],</span>
    <span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">start_time</span><span class="o">=</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1">#these will get normalized but it should still work</span>
<span class="n">input_features_new</span> <span class="o">=</span> <span class="n">input_features</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">,</span><span class="s1">&#39;TX_DATETIME_TIMESTAMP&#39;</span><span class="p">]</span>

<span class="n">performances_df</span><span class="o">=</span><span class="n">model_selection_wrapper</span><span class="p">(</span><span class="n">transactions_df</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> 
                                        <span class="n">input_features_new</span><span class="p">,</span> <span class="n">output_feature</span><span class="p">,</span>
                                        <span class="n">parameters</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> 
                                        <span class="n">start_date_training_for_valid</span><span class="p">,</span>
                                        <span class="n">start_date_training_for_test</span><span class="p">,</span>
                                        <span class="n">n_folds</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span>
                                        <span class="n">delta_train</span><span class="o">=</span><span class="n">delta_train</span><span class="p">,</span> 
                                        <span class="n">delta_delay</span><span class="o">=</span><span class="n">delta_delay</span><span class="p">,</span> 
                                        <span class="n">delta_assessment</span><span class="o">=</span><span class="n">delta_assessment</span><span class="p">,</span>
                                        <span class="n">performance_metrics_list_grid</span><span class="o">=</span><span class="n">performance_metrics_list_grid</span><span class="p">,</span>
                                        <span class="n">performance_metrics_list</span><span class="o">=</span><span class="n">performance_metrics_list</span><span class="p">,</span>
                                        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">execution_time_lstm</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span>

<span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                    <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>                           
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>

<span class="c1"># Rename to performances_df_lstm for model performance comparison at the end of this notebook</span>
<span class="n">performances_df_lstm</span><span class="o">=</span><span class="n">performances_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">execution_time_lstm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14750.79782986641
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summary_performances_lstm</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">summary_performances_lstm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AUC ROC</th>
      <th>Average precision</th>
      <th>Card Precision@100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Best estimated parameters</th>
      <td>5/256/100/0.2</td>
      <td>20/128/100/0.4</td>
      <td>5/64/200/0</td>
    </tr>
    <tr>
      <th>Validation performance</th>
      <td>0.881+/-0.01</td>
      <td>0.681+/-0.01</td>
      <td>0.286+/-0.02</td>
    </tr>
    <tr>
      <th>Test performance</th>
      <td>0.876+/-0.02</td>
      <td>0.665+/-0.02</td>
      <td>0.297+/-0.01</td>
    </tr>
    <tr>
      <th>Optimal parameter(s)</th>
      <td>5/256/200/0.4</td>
      <td>20/128/100/0.2</td>
      <td>5/64/200/0.2</td>
    </tr>
    <tr>
      <th>Optimal test performance</th>
      <td>0.878+/-0.01</td>
      <td>0.674+/-0.01</td>
      <td>0.302+/-0.01</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The results with the LSTM are more competitive than the CNN, and they are similar to the feed-forward network. This is still aligned with the hypothesis that the sequence does not really provide, for this dataset, a notable added value compared to only the last transaction with aggregated information. Nevertheless, the comparison with the CNN shows that, even with the same input, the choice of architecture can have an important impact on the performance. Moreover, the performed grid-search is for illustration purposes and is far from exhaustive, so the hyperparameters tuning brings an additional artifact to the results.</p>
<p>The conclusions about the set of optimal hyperparameters are very mixed here. Let us fix the learning rate to 0.0001, the batch size to 128, the dropout level to 0.2, and visualize the impact of the number of epochs and of the size of the hidden states.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df_lstm</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df_lstm</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>

<span class="n">performances_df_lstm_subset</span> <span class="o">=</span> <span class="n">performances_df_lstm</span><span class="p">[</span><span class="n">performances_df_lstm</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__lr&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mf">0.0001</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">128</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">10</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">summary_performances_lstm_subset</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm_subset</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">indexes_summary</span> <span class="o">=</span> <span class="n">summary_performances_lstm_subset</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
<span class="n">indexes_summary</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Best estimated parameters&#39;</span>
<span class="n">summary_performances_lstm_subset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes_summary</span><span class="p">)),</span><span class="n">indexes_summary</span><span class="p">)))</span>
<span class="n">get_performances_plots</span><span class="p">(</span><span class="n">performances_df_lstm_subset</span><span class="p">,</span> 
                       <span class="n">performance_metrics_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">],</span> 
                       <span class="n">expe_type_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">expe_type_color_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#008000&#39;</span><span class="p">,</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">],</span>
                       <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;hidden states size&quot;</span><span class="p">,</span>
                       <span class="n">summary_performances</span><span class="o">=</span><span class="n">summary_performances_lstm_subset</span><span class="p">)</span>

<span class="n">performances_df_lstm_subset</span> <span class="o">=</span> <span class="n">performances_df_lstm</span><span class="p">[</span><span class="n">performances_df_lstm</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__lr&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mf">0.0001</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">128</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">100</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">summary_performances_lstm_subset</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm_subset</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">indexes_summary</span> <span class="o">=</span> <span class="n">summary_performances_lstm_subset</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
<span class="n">indexes_summary</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Best estimated parameters&#39;</span>
<span class="n">summary_performances_lstm_subset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes_summary</span><span class="p">)),</span><span class="n">indexes_summary</span><span class="p">)))</span>
<span class="n">get_performances_plots</span><span class="p">(</span><span class="n">performances_df_lstm_subset</span><span class="p">,</span> 
                       <span class="n">performance_metrics_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">],</span> 
                       <span class="n">expe_type_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">expe_type_color_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#008000&#39;</span><span class="p">,</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">],</span>
                       <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span>
                       <span class="n">summary_performances</span><span class="o">=</span><span class="n">summary_performances_lstm_subset</span><span class="p">)</span>

<span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df_lstm</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df_lstm</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                    <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>                           
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/SequentialModeling_131_0.png" src="../_images/SequentialModeling_131_0.png" />
<img alt="../_images/SequentialModeling_131_1.png" src="../_images/SequentialModeling_131_1.png" />
</div>
</div>
<p>Recall here that the grid search is not exhaustive, and from the plots, it appears that the average precision could have been further improved.</p>
</div>
<div class="section" id="grid-search-on-the-lstm-with-attention">
<h2><span class="section-number">4.9. </span>Grid search on the LSTM with Attention<a class="headerlink" href="#grid-search-on-the-lstm-with-attention" title="Permalink to this headline">¶</a></h2>
<p>For the LSTM with Attention, we apply the exact same process as for the LSTM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FraudLSTMWithAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">hidden_size_lstm</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num_layers_lstm</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FraudLSTMWithAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># representation learning part</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">hidden_size_lstm</span><span class="p">,</span> <span class="n">num_layers_lstm</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="n">p</span><span class="p">)</span>
            
        <span class="c1"># last sequence represenation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span><span class="n">hidden_size_lstm</span><span class="p">)</span>
        
        <span class="n">attention_out_dim</span> <span class="o">=</span> <span class="n">hidden_size_lstm</span>
        
        <span class="c1"># attention layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">attention_out_dim</span><span class="p">)</span>                        
       
        <span class="c1">#representation to hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size_lstm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        
        <span class="c1">#hidden to output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="n">representation_seq</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
        
        <span class="n">representation_last</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">[:,:,</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
        
        <span class="n">representation</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">representation_last</span><span class="p">,</span><span class="n">representation_seq</span><span class="p">)</span>

                    
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">representation</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:])</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_everything</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">NeuralNetClassifier</span><span class="p">(</span>
    <span class="n">FraudLSTMWithAttention</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">FraudSequenceDatasetForPipe</span><span class="p">,</span>
    <span class="n">iterator_train__shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">train_split</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;clf__lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span><span class="mf">0.0002</span><span class="p">,</span><span class="mf">0.001</span><span class="p">],</span>
    <span class="s1">&#39;clf__batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span>
    <span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__hidden_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">500</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__p&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.4</span><span class="p">],</span>
    <span class="s1">&#39;clf__module__num_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_features</span><span class="p">))],</span>
    <span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">start_time</span><span class="o">=</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1">#these will get normalized but it should still work</span>
<span class="n">input_features_new</span> <span class="o">=</span> <span class="n">input_features</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;CUSTOMER_ID&#39;</span><span class="p">,</span><span class="s1">&#39;TX_DATETIME_TIMESTAMP&#39;</span><span class="p">]</span>

<span class="n">performances_df</span><span class="o">=</span><span class="n">model_selection_wrapper</span><span class="p">(</span><span class="n">transactions_df</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> 
                                        <span class="n">input_features_new</span><span class="p">,</span> <span class="n">output_feature</span><span class="p">,</span>
                                        <span class="n">parameters</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> 
                                        <span class="n">start_date_training_for_valid</span><span class="p">,</span>
                                        <span class="n">start_date_training_for_test</span><span class="p">,</span>
                                        <span class="n">n_folds</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span>
                                        <span class="n">delta_train</span><span class="o">=</span><span class="n">delta_train</span><span class="p">,</span> 
                                        <span class="n">delta_delay</span><span class="o">=</span><span class="n">delta_delay</span><span class="p">,</span> 
                                        <span class="n">delta_assessment</span><span class="o">=</span><span class="n">delta_assessment</span><span class="p">,</span>
                                        <span class="n">performance_metrics_list_grid</span><span class="o">=</span><span class="n">performance_metrics_list_grid</span><span class="p">,</span>
                                        <span class="n">performance_metrics_list</span><span class="o">=</span><span class="n">performance_metrics_list</span><span class="p">,</span>
                                        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">execution_time_lstm_attn</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span>

<span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                    <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>                           
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>

<span class="c1"># Rename to performances_df_lstm for model performance comparison at the end of this notebook</span>
<span class="n">performances_df_lstm_attn</span><span class="o">=</span><span class="n">performances_df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">execution_time_lstm_attn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>16735.779472112656
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summary_performances_lstm_attn</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm_attn</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">summary_performances_lstm_attn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AUC ROC</th>
      <th>Average precision</th>
      <th>Card Precision@100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Best estimated parameters</th>
      <td>10/64/100/0.2</td>
      <td>10/64/200/0.2</td>
      <td>20/256/200/0.2</td>
    </tr>
    <tr>
      <th>Validation performance</th>
      <td>0.886+/-0.02</td>
      <td>0.69+/-0.01</td>
      <td>0.287+/-0.02</td>
    </tr>
    <tr>
      <th>Test performance</th>
      <td>0.874+/-0.02</td>
      <td>0.667+/-0.01</td>
      <td>0.295+/-0.01</td>
    </tr>
    <tr>
      <th>Optimal parameter(s)</th>
      <td>5/256/200/0.4</td>
      <td>10/64/200/0</td>
      <td>10/64/200/0</td>
    </tr>
    <tr>
      <th>Optimal test performance</th>
      <td>0.879+/-0.02</td>
      <td>0.676+/-0.02</td>
      <td>0.303+/-0.01</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The LSTM with Attention has a performance that is only slightly better than the regular LSTM. Although Attention is a mechanism that significantly impacts many applications such as NLP, its interest can be limited on such sequences that are rather short and on such a model that only has a single recurrent layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>

<span class="n">performances_df_lstm_attn_subset</span> <span class="o">=</span> <span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__lr&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mf">0.0001</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">128</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">10</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">summary_performances_lstm_attn_subset</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm_attn_subset</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">indexes_summary</span> <span class="o">=</span> <span class="n">summary_performances_lstm_attn_subset</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
<span class="n">indexes_summary</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Best estimated parameters&#39;</span>
<span class="n">summary_performances_lstm_attn_subset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes_summary</span><span class="p">)),</span><span class="n">indexes_summary</span><span class="p">)))</span>
<span class="n">get_performances_plots</span><span class="p">(</span><span class="n">performances_df_lstm_attn_subset</span><span class="p">,</span> 
                       <span class="n">performance_metrics_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">],</span> 
                       <span class="n">expe_type_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">expe_type_color_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#008000&#39;</span><span class="p">,</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">],</span>
                       <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;hidden states size&quot;</span><span class="p">,</span>
                       <span class="n">summary_performances</span><span class="o">=</span><span class="n">summary_performances_lstm_attn_subset</span><span class="p">)</span>

<span class="n">performances_df_lstm_attn_subset</span> <span class="o">=</span> <span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__lr&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mf">0.0001</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">128</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">100</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">summary_performances_lstm_attn_subset</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm_attn_subset</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>
<span class="n">indexes_summary</span> <span class="o">=</span> <span class="n">summary_performances_lstm_attn_subset</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
<span class="n">indexes_summary</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Best estimated parameters&#39;</span>
<span class="n">summary_performances_lstm_attn_subset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes_summary</span><span class="p">)),</span><span class="n">indexes_summary</span><span class="p">)))</span>
<span class="n">get_performances_plots</span><span class="p">(</span><span class="n">performances_df_lstm_attn_subset</span><span class="p">,</span> 
                       <span class="n">performance_metrics_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AUC ROC&#39;</span><span class="p">,</span> <span class="s1">&#39;Average precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Card Precision@100&#39;</span><span class="p">],</span> 
                       <span class="n">expe_type_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">expe_type_color_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#008000&#39;</span><span class="p">,</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">],</span>
                       <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span>
                       <span class="n">summary_performances</span><span class="o">=</span><span class="n">summary_performances_lstm_attn_subset</span><span class="p">)</span>

<span class="n">parameters_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="s1">&#39;Parameters&#39;</span><span class="p">])</span>
<span class="n">performances_df_lstm_attn</span><span class="p">[</span><span class="s1">&#39;Parameters summary&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__max_epochs&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                    <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__batch_size&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__hidden_size_lstm&#39;</span><span class="p">])</span><span class="o">+</span>
                                   <span class="s1">&#39;/&#39;</span><span class="o">+</span>                           
                                   <span class="nb">str</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;clf__module__p&#39;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_dict</span><span class="p">))]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/SequentialModeling_139_0.png" src="../_images/SequentialModeling_139_0.png" />
<img alt="../_images/SequentialModeling_139_1.png" src="../_images/SequentialModeling_139_1.png" />
</div>
</div>
<p>For the same set of hyperparameters, the convergence w.r.t the average precision is different than LSTM without Attention.</p>
</div>
<div class="section" id="saving-of-results">
<h2><span class="section-number">4.10. </span>Saving of results<a class="headerlink" href="#saving-of-results" title="Permalink to this headline">¶</a></h2>
<p>Let us save the performance results and execution times of these three models in a Python pickle format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">performances_df_dictionary</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;CNN&quot;</span><span class="p">:</span> <span class="n">performances_df_cnn</span><span class="p">,</span>    
    <span class="s2">&quot;LSTM&quot;</span><span class="p">:</span> <span class="n">performances_df_lstm</span><span class="p">,</span>    
    <span class="s2">&quot;LSTM_Attention&quot;</span><span class="p">:</span> <span class="n">performances_df_lstm_attn</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">execution_times</span><span class="o">=</span><span class="p">[</span><span class="n">execution_time_cnn</span><span class="p">,</span><span class="n">execution_time_lstm</span><span class="p">,</span><span class="n">execution_time_lstm_attn</span><span class="p">]</span>

<span class="n">filehandler</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;performances_model_selection_seq_model.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> 
<span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">((</span><span class="n">performances_df_dictionary</span><span class="p">,</span> <span class="n">execution_times</span><span class="p">),</span> <span class="n">filehandler</span><span class="p">)</span>
<span class="n">filehandler</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="benchmark-summary">
<h2><span class="section-number">4.11. </span>Benchmark summary<a class="headerlink" href="#benchmark-summary" title="Permalink to this headline">¶</a></h2>
<p>Let us finally retrieve the performance results obtained in</p>
<ul class="simple">
<li><p><a class="reference internal" href="../Chapter_5_ModelValidationAndSelection/ModelSelection.html#model-selection-comparison-performances"><span class="std std-ref">Chapter 5</span></a> with decision tree, logistic regression, random forest and XGBoost, and</p></li>
<li><p><a class="reference internal" href="FeedForwardNeuralNetworks.html#model-selection-ffnn"><span class="std std-ref">Chapter 7, Section 2</span></a> with feed-forward neural networks</p></li>
</ul>
<p>and compare them with those obtained with CNN, LSTM, and LSTM with Attention.</p>
<p>The results can be retrieved by loading the <code class="docutils literal notranslate"><span class="pre">performances_model_selection.pkl</span></code>, <code class="docutils literal notranslate"><span class="pre">performances_model_selection_nn.pkl</span></code> and <code class="docutils literal notranslate"><span class="pre">performances_model_selection_seq_model.pkl</span></code> pickle files, and summarized with the <code class="docutils literal notranslate"><span class="pre">get_summary_performances</span></code> function.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load performance results for decision tree, logistic regression, random forest and XGBoost</span>
<span class="n">filehandler</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../Chapter_5_ModelValidationAndSelection/performances_model_selection.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> 
<span class="p">(</span><span class="n">performances_df_dictionary</span><span class="p">,</span> <span class="n">execution_times</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filehandler</span><span class="p">)</span>

<span class="c1"># Load performance results for feed-forward neural network</span>
<span class="n">filehandler</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;performances_model_selection_nn.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> 
<span class="p">(</span><span class="n">performances_df_dictionary_nn</span><span class="p">,</span> <span class="n">execution_times_nn</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filehandler</span><span class="p">)</span>

<span class="c1"># Load performance results for CNN, LSTM and LSTM with Attention</span>
<span class="n">filehandler</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;performances_model_selection_seq_model.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> 
<span class="p">(</span><span class="n">performances_df_dictionary_seq_model</span><span class="p">,</span> <span class="n">execution_times_seq_model</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filehandler</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">performances_df_dt</span><span class="o">=</span><span class="n">performances_df_dictionary</span><span class="p">[</span><span class="s1">&#39;Decision Tree&#39;</span><span class="p">]</span>
<span class="n">summary_performances_dt</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_dt</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">performances_df_lr</span><span class="o">=</span><span class="n">performances_df_dictionary</span><span class="p">[</span><span class="s1">&#39;Logistic Regression&#39;</span><span class="p">]</span>
<span class="n">summary_performances_lr</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lr</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">performances_df_rf</span><span class="o">=</span><span class="n">performances_df_dictionary</span><span class="p">[</span><span class="s1">&#39;Random Forest&#39;</span><span class="p">]</span>
<span class="n">summary_performances_rf</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_rf</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">performances_df_xgboost</span><span class="o">=</span><span class="n">performances_df_dictionary</span><span class="p">[</span><span class="s1">&#39;XGBoost&#39;</span><span class="p">]</span>
<span class="n">summary_performances_xgboost</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_xgboost</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">performances_df_nn</span><span class="o">=</span><span class="n">performances_df_dictionary_nn</span><span class="p">[</span><span class="s1">&#39;Neural Network&#39;</span><span class="p">]</span>
<span class="n">summary_performances_nn</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_nn</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">performances_df_cnn</span><span class="o">=</span><span class="n">performances_df_dictionary_seq_model</span><span class="p">[</span><span class="s1">&#39;CNN&#39;</span><span class="p">]</span>
<span class="n">summary_performances_cnn</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_cnn</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">performances_df_lstm</span><span class="o">=</span><span class="n">performances_df_dictionary_seq_model</span><span class="p">[</span><span class="s1">&#39;LSTM&#39;</span><span class="p">]</span>
<span class="n">summary_performances_lstm</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">performances_df_lstm_attention</span><span class="o">=</span><span class="n">performances_df_dictionary_seq_model</span><span class="p">[</span><span class="s1">&#39;LSTM_Attention&#39;</span><span class="p">]</span>
<span class="n">summary_performances_lstm_attention</span><span class="o">=</span><span class="n">get_summary_performances</span><span class="p">(</span><span class="n">performances_df_lstm_attention</span><span class="p">,</span> <span class="n">parameter_column_name</span><span class="o">=</span><span class="s2">&quot;Parameters summary&quot;</span><span class="p">)</span>

<span class="n">summary_test_performances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">summary_performances_dt</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                       <span class="n">summary_performances_lr</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                       <span class="n">summary_performances_rf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                       <span class="n">summary_performances_xgboost</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                       <span class="n">summary_performances_nn</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                       <span class="n">summary_performances_cnn</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                       <span class="n">summary_performances_lstm</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                       <span class="n">summary_performances_lstm_attention</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span>
                                      <span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">summary_test_performances</span><span class="o">.</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Decision Tree&#39;</span><span class="p">,</span> <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="s1">&#39;Random Forest&#39;</span><span class="p">,</span> <span class="s1">&#39;XGBoost&#39;</span><span class="p">,</span> 
                                   <span class="s1">&#39;Neural Network&#39;</span><span class="p">,</span> <span class="s1">&#39;CNN&#39;</span><span class="p">,</span> <span class="s1">&#39;LSTM&#39;</span><span class="p">,</span> <span class="s1">&#39;LSTM with Attention&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The results are summarized in a <code class="docutils literal notranslate"><span class="pre">summary_test_performances</span></code> table. Rows provide the average performance results on the test sets in terms of AUC ROC, Average Precision and CP&#64;100.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summary_test_performances</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Decision Tree</th>
      <th>Logistic Regression</th>
      <th>Random Forest</th>
      <th>XGBoost</th>
      <th>Neural Network</th>
      <th>CNN</th>
      <th>LSTM</th>
      <th>LSTM with Attention</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AUC ROC</th>
      <td>0.797+/-0.01</td>
      <td>0.868+/-0.02</td>
      <td>0.87+/-0.02</td>
      <td>0.869+/-0.01</td>
      <td>0.876+/-0.01</td>
      <td>0.872+/-0.01</td>
      <td>0.876+/-0.02</td>
      <td>0.874+/-0.02</td>
    </tr>
    <tr>
      <th>Average precision</th>
      <td>0.579+/-0.01</td>
      <td>0.623+/-0.02</td>
      <td>0.678+/-0.01</td>
      <td>0.687+/-0.01</td>
      <td>0.675+/-0.01</td>
      <td>0.599+/-0.01</td>
      <td>0.665+/-0.02</td>
      <td>0.667+/-0.01</td>
    </tr>
    <tr>
      <th>Card Precision@100</th>
      <td>0.284+/-0.0</td>
      <td>0.297+/-0.01</td>
      <td>0.299+/-0.01</td>
      <td>0.303+/-0.01</td>
      <td>0.303+/-0.02</td>
      <td>0.288+/-0.01</td>
      <td>0.297+/-0.01</td>
      <td>0.295+/-0.01</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In the end, it appears that neural-network based approaches provide results that are competitive with random forests and XGBoost, providing slightly better performances in terms of AUC ROC, and slightly worse performances in terms of Average Precision. However, it should be kept in mind that, given more computational resources, a more extensive hyperparameter tuning could be performed for neural network-based approaches that would likely lead to further improve their performances.</p>
</div>
<div class="section" id="conclusion">
<h2><span class="section-number">4.12. </span>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this section, automatic methods to build features from contextual data were explored. To classify a transaction as fraudulent or genuine, it is generally useful to resort to the regular behavior of the cardholder in order to detect a discrepancy. A manual method to integrate this contextual information is to proceed with feature engineering and the creation of expert feature aggregations.</p>
<p>Automatic representation learning methods from the Deep Learning literature have been explored to create features that represent the cardholder’s sequence of previous transactions in a way that optimizes the classification of the current transaction.</p>
<p>The key methodological components are the building of the data pipeline to automatically create the sequential data from landmark variables and the right combination of adapted neural networks such as the 1D-CNN, the LSTM, the Attention mechanism, etc. These models have been tested here, and they can provide competitive performance. In the future, other candidate architectures such as Multi-Head Self-Attention, Sequence-to-Sequence Autoencoder, or any combination of the above modules could be explored to attempt to further improve fraud detection performance.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Chapter_7_DeepLearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Autoencoders.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3. </span>Autoencoders and anomaly detection</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="RealWorldData.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Real-world data</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By <a href="https://mlg.ulb.ac.be/wordpress/">Machine Learning Group (Université Libre de Bruxelles - ULB)</a>.<br/>
        
          <div class="extra_footer">
            <p>
Code released under a <a href="https://www.gnu.org/licenses/gpl-3.0.en.html">GNU GPL v3.0 license</a>. 
Prose and pictures released under a <a href="https://creativecommons.org/licenses/by-sa/4.0/"> CC BY-SA 4.0 license</a>.
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>